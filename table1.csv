name|year|accuracy|paper
FireCaffe |2015|0.6829999999999999|firecaffe-near-linear-acceleration-of-deep
ResNet-152|2015|0.7857|deep-residual-learning-for-image-recognition
Inception ResNet V2|2016|0.8009999999999999|inception-v4-inception-resnet-and-the-impact
SimpleNetV1-9m-correct-labels|2016|0.8123999999999999|lets-keep-it-simple-using-simple
NASNET-A|2017|0.8270000000000001|learning-transferable-architectures-for
PNASNet-5|2017|0.8290000000000001|progressive-neural-architecture-search
AmoebaNet-A|2018|0.8390000000000001|regularized-evolution-for-image-classifier
ResNeXt-101 32x48d|2018|0.8540000000000001|exploring-the-limits-of-weakly-supervised
FixResNeXt-101 32x48d|2019|0.8640000000000001|fixing-the-train-test-resolution-discrepancy
NoisyStudent |2019|0.8690000000000001|self-training-with-noisy-student-improves
BiT-L |2019|0.8754000000000001|large-scale-learning-of-general-visual
NoisyStudent |2020|0.884|self-training-with-noisy-student-improves
FixEfficientNet-L2|2020|0.885|fixing-the-train-test-resolution-discrepancy-2
Meta Pseudo Labels |2020|0.902|meta-pseudo-labels
ViT-G/14|2021|0.9045000000000001|scaling-vision-transformers
CoAtNet-7|2021|0.9087999999999999|coatnet-marrying-convolution-and-attention
Model soups |2022|0.9098|model-soups-averaging-weights-of-multiple
CoCa |2022|0.91|coca-contrastive-captioners-are-image-text
OmniVec|2023|0.924|omnivec-learning-robust-representations-with
ru|"""nam"|"""scatter"""|{name
ResNet-101|2015|0.7825|deep-residual-learning-for-image-recognition
ResNet-50|2015|0.753|deep-residual-learning-for-image-recognition
ResNet-200|2016|0.799|identity-mappings-in-deep-residual-networks
WRN-50-2-bottleneck|2016|0.7809999999999999|wide-residual-networks
FractalNet-34|2016|0.7587999999999999|fractalnet-ultra-deep-neural-networks-without
SimpleNetV1-5m-correct-labels|2016|0.7912|lets-keep-it-simple-using-simple
SimpleNetV1-small-075-correct-labels|2016|0.7565999999999999|lets-keep-it-simple-using-simple
SimpleNetV1-9m|2016|0.7417|lets-keep-it-simple-using-simple
SimpleNetV1-5m|2016|0.7193999999999999|lets-keep-it-simple-using-simple
SimpleNetV1-small-05-correct-labels|2016|0.6911|lets-keep-it-simple-using-simple
SimpleNetV1-small-075|2016|0.6815000000000001|lets-keep-it-simple-using-simple
SimpleNetV1-small-05|2016|0.6152000000000001|lets-keep-it-simple-using-simple
DenseNet-264|2016|0.7785|densely-connected-convolutional-networks
DenseNet-201|2016|0.7742|densely-connected-convolutional-networks
DenseNet-169|2016|0.762|densely-connected-convolutional-networks
DenseNet-121|2016|0.7498|densely-connected-convolutional-networks
Xception|2016|0.79|xception-deep-learning-with-depthwise
ResNeXt-101  64x4|2016|0.809|aggregated-residual-transformations-for-deep
MobileNet-224 \u00d71.25|2017|0.706|mobilenets-efficient-convolutional-neural
Attention-92|2017|0.805|residual-attention-network-for-image
ShuffleNet|2017|0.7090000000000001|shufflenet-an-extremely-efficient
ResNet-101 |2017|0.792|revisiting-unreasonable-effectiveness-of-data
MobileNetV2 |2018|0.747|mobilenetv2-inverted-residuals-and-linear
MobileNetV2|2018|0.72|mobilenetv2-inverted-residuals-and-linear
ResNet-152 + SWA|2018|0.7894|averaging-weights-leads-to-wider-optima-and
DenseNet-161 + SWA|2018|0.7844|averaging-weights-leads-to-wider-optima-and
Inception v3|2018|0.7712|what-do-deep-networks-like-to-see
ResNeXt-101 32x32d|2018|0.851|exploring-the-limits-of-weakly-supervised
ResNeXt-101 32\u00d716d|2018|0.8420000000000001|exploring-the-limits-of-weakly-supervised
ResNeXt-101 32x8d|2018|0.8220000000000001|exploring-the-limits-of-weakly-supervised
CoordConv ResNet-50|2018|0.7574|an-intriguing-failing-of-convolutional-neural
ShuffleNet V2|2018|0.754|shufflenet-v2-practical-guidelines-for
MnasNet-A3|2018|0.767|mnasnet-platform-aware-neural-architecture
MnasNet-A2|2018|0.7559999999999999|mnasnet-platform-aware-neural-architecture
MnasNet-A1|2018|0.752|mnasnet-platform-aware-neural-architecture
GPIPE|2018|0.8440000000000001|gpipe-efficient-training-of-giant-neural
ESPNetv2|2018|0.7490000000000001|espnetv2-a-light-weight-power-efficient-and
Proxyless|2018|0.746|proxylessnas-direct-neural-architecture
ResNet-50-D|2018|0.7716|bag-of-tricks-for-image-classification-with
FBNet-C|2018|0.7490000000000001|fbnet-hardware-aware-efficient-convnet-design
ColorNet |2019|0.8432|colornet-investigating-the-importance-of
ColorNet|2019|0.8234999999999999|colornet-investigating-the-importance-of
MultiGrain PNASNet |2019|0.836|multigrain-a-unified-image-embedding-for
MultiGrain PNASNet |2019|0.8320000000000001|multigrain-a-unified-image-embedding-for
MultiGrain SENet154 |2019|0.831|multigrain-a-unified-image-embedding-for
MultiGrain SENet154 |2019|0.83|multigrain-a-unified-image-embedding-for
MultiGrain SENet154 |2019|0.8270000000000001|multigrain-a-unified-image-embedding-for
MultiGrain PNASNet |2019|0.826|multigrain-a-unified-image-embedding-for
MultiGrain PNASNet |2019|0.813|multigrain-a-unified-image-embedding-for
MultiGrain R50-AA-500|2019|0.794|multigrain-a-unified-image-embedding-for
MultiGrain R50-AA-224|2019|0.782|multigrain-a-unified-image-embedding-for
MultiGrain NASNet-A-Mobile |2019|0.7509999999999999|multigrain-a-unified-image-embedding-for
Graph-RISE |2019|0.6829000000000001|graph-rise-graph-regularized-image-semantic
SKNet-101|2019|0.7981|selective-kernel-networks
SRM-ResNet-101|2019|0.7847|srm-a-style-based-recalibration-module-for
Res2Net-101|2019|0.8123|res2net-a-new-multi-scale-backbone
RandWire-WS|2019|0.8009999999999999|exploring-randomly-wired-neural-networks-for
Res2Net-50-299|2019|0.7859|res2net-a-new-multi-scale-backbone
RandWire-WS |2019|0.747|exploring-randomly-wired-neural-networks-for
Single-Path NAS|2019|0.7495999999999999|single-path-nas-designing-hardware-efficient
ACNet |2019|0.775|adaptively-connected-neural-networks
Oct-ResNet-152 |2019|0.8290000000000001|drop-an-octave-reducing-spatial-redundancy-in
EfficientNet-B0 |2019|0.7829999999999999|soft-conditional-computation
ScaleNet-152|2019|0.7938|190409460
ScaleNet-101|2019|0.7903|190409460
ScaleNet-50|2019|0.778|190409460
AA-ResNet-152|2019|0.7909999999999999|190409925
LR-Net-26|2019|0.757|190411491
ResNet-50 |2019|0.7904000000000001|unsupervised-data-augmentation-1
ResNet-200 |2019|0.8059999999999999|fast-autoaugment
ResNet-50 |2019|0.7759999999999999|fast-autoaugment
ResNeXt-101 32x16d |2019|0.848|billion-scale-semi-supervised-learning-for
ResNeXt-101 32x8d |2019|0.843|billion-scale-semi-supervised-learning-for
ResNeXt-101 32x4d |2019|0.8340000000000001|billion-scale-semi-supervised-learning-for
MobileNet V3-Large 1.0|2019|0.752|searching-for-mobilenetv3
ResNeXt-101 |2019|0.8053|cutmix-regularization-strategy-to-train
ResNet-50 |2019|0.784|cutmix-regularization-strategy-to-train
SGE-ResNet101|2019|0.78798|spatial-group-wise-enhance-improving-semantic
SGE-ResNet50|2019|0.7758400000000001|spatial-group-wise-enhance-improving-semantic
EfficientNet-B7|2019|0.8440000000000001|efficientnet-rethinking-model-scaling-for
EfficientNet-B6|2019|0.84|efficientnet-rethinking-model-scaling-for
EfficientNet-B5|2019|0.833|efficientnet-rethinking-model-scaling-for
EfficientNet-B4|2019|0.826|efficientnet-rethinking-model-scaling-for
EfficientNet-B3|2019|0.8109999999999999|efficientnet-rethinking-model-scaling-for
EfficientNet-B2|2019|0.7979999999999999|efficientnet-rethinking-model-scaling-for
EfficientNet-B1|2019|0.7879999999999999|efficientnet-rethinking-model-scaling-for
EfficientNet-B0|2019|0.763|efficientnet-rethinking-model-scaling-for
DiCENet|2019|0.7509999999999999|dicenet-dimension-wise-convolutions-for
FixResNet-50 Billion-scale@224|2019|0.825|fixing-the-train-test-resolution-discrepancy
FixResNet-50 CutMix|2019|0.7979999999999999|fixing-the-train-test-resolution-discrepancy
FixResNet-50|2019|0.7909999999999999|fixing-the-train-test-resolution-discrepancy
DenseNAS-A|2019|0.759|densely-connected-search-space-for-more
FairNAS-A|2019|0.7534000000000001|fairnas-rethinking-evaluation-fairness-of
FairNAS-B|2019|0.7509999999999999|fairnas-rethinking-evaluation-fairness-of
FairNAS-C|2019|0.7469|fairnas-rethinking-evaluation-fairness-of
MixNet-L|2019|0.789|mixnet-mixed-depthwise-convolutional-kernels
MixNet-M|2019|0.77|mixnet-mixed-depthwise-convolutional-kernels
MixNet-S|2019|0.758|mixnet-mixed-depthwise-convolutional-kernels
MobileNet-224 |2019|0.7256|compact-global-descriptor-for-neural-networks
AOGNet-40M-AN|2019|0.8187000000000001|attentive-normalization
MoGA-A|2019|0.759|moga-searching-beyond-mobilenetv3
LIP-ResNet-101|2019|0.7933|lip-local-importance-based-pooling
ResNet-50 |2019|0.7815000000000001|lip-local-importance-based-pooling
LIP-DenseNet-BC-121|2019|0.7664|lip-local-importance-based-pooling
SCARLET-A4|2019|0.823|scarletnas-bridging-the-gap-between
SCARLET-A|2019|0.769|scarletnas-bridging-the-gap-between
SCARLET-B|2019|0.763|scarletnas-bridging-the-gap-between
SCARLET-C|2019|0.7559999999999999|scarletnas-bridging-the-gap-between
CSPResNeXt-50 + Mish|2019|0.7979999999999999|mish-a-self-regularized-non-monotonic-neural
HCGNet-C|2019|0.805|gated-convolutional-networks-with-hybrid
HCGNet-B|2019|0.785|gated-convolutional-networks-with-hybrid
BBG |2019|0.626|balanced-binary-neural-networks-with-gated
BBG |2019|0.594|balanced-binary-neural-networks-with-gated
EfficientNet-B8 |2019|0.8540000000000001|randaugment-practical-data-augmentation-with
EfficientNet-B7 |2019|0.85|randaugment-practical-data-augmentation-with
ResNet-50-DW |2019|0.785|deformable-kernels-adapting-effective
ECA-Net |2019|0.7892|eca-net-efficient-channel-attention-for-deep
ECA-Net |2019|0.7865000000000001|eca-net-efficient-channel-attention-for-deep
ECA-Net |2019|0.7748|eca-net-efficient-channel-attention-for-deep
ECA-Net |2019|0.7256|eca-net-efficient-channel-attention-for-deep
ResNet-50|2019|0.721|on-the-adequacy-of-untuned-warmup-for
NoisyStudent |2019|0.8640000000000001|self-training-with-noisy-student-improves
NoisyStudent |2019|0.861|self-training-with-noisy-student-improves
NoisyStudent |2019|0.853|self-training-with-noisy-student-improves
NoisyStudent |2019|0.841|self-training-with-noisy-student-improves
NoisyStudent |2019|0.8240000000000001|self-training-with-noisy-student-improves
NoisyStudent |2019|0.815|self-training-with-noisy-student-improves
NoisyStudent |2019|0.7879999999999999|self-training-with-noisy-student-improves
AdvProp |2019|0.855|adversarial-examples-improve-image
AdvProp |2019|0.852|adversarial-examples-improve-image
InceptionV3 |2019|0.7895|filter-response-normalization-layer
ResnetV2 50 |2019|0.7720999999999999|filter-response-normalization-layer
CSPResNeXt-50 |2019|0.7979999999999999|cspnet-a-new-backbone-that-can-enhance
GhostNet \u00d71.3|2019|0.757|ghostnet-more-features-from-cheap-operations
Ghost-ResNet-50 |2019|0.75|ghostnet-more-features-from-cheap-operations
Ghost-ResNet-50 |2019|0.741|ghostnet-more-features-from-cheap-operations
GhostNet \u00d71.0|2019|0.7390000000000001|ghostnet-more-features-from-cheap-operations
GhostNet \u00d70.5|2019|0.662|ghostnet-more-features-from-cheap-operations
Wide ResNet-50 |2019|0.733|whats-hidden-in-a-randomly-weighted-neural
DY-MobileNetV2 \u00d71.0|2019|0.7440000000000001|dynamic-convolution-attention-over
DY-MobileNetV2 \u00d70.75|2019|0.728|dynamic-convolution-attention-over
DY-ResNet-18|2019|0.727|dynamic-convolution-attention-over
DY-MobileNetV3-Small|2019|0.6970000000000001|dynamic-convolution-attention-over
DY-MobileNetV2 \u00d70.5|2019|0.6940000000000001|dynamic-convolution-attention-over
DY-ResNet-10|2019|0.677|dynamic-convolution-attention-over
DY-MobileNetV2 \u00d70.35|2019|0.649|dynamic-convolution-attention-over
SpineNet-143|2019|0.79|spinenet-learning-scale-permuted-backbone-for
BiT-M |2019|0.8539|large-scale-learning-of-general-visual
ResNet-200 |2019|0.8131999999999999|adversarial-autoaugment-1
ResNet-50 |2019|0.794|adversarial-autoaugment-1
Assemble-ResNet152|2020|0.8420000000000001|compounding-the-performance-improvements-of
Fix-EfficientNet-B8 |2020|0.858|maxup-a-simple-way-to-improve-generalization
FixEfficientNet-B7|2020|0.871|fixing-the-train-test-resolution-discrepancy-2
FixEfficientNet-B6|2020|0.867|fixing-the-train-test-resolution-discrepancy-2
FixEfficientNet-B5|2020|0.8640000000000001|fixing-the-train-test-resolution-discrepancy-2
FixEfficientNet-B4|2020|0.8590000000000001|fixing-the-train-test-resolution-discrepancy-2
FixEfficientNet-B8|2020|0.857|fixing-the-train-test-resolution-discrepancy-2
FixEfficientNet-B3|2020|0.85|fixing-the-train-test-resolution-discrepancy-2
FixEfficientNetB4|2020|0.84|fixing-the-train-test-resolution-discrepancy-2
FixEfficientNet-B2|2020|0.836|fixing-the-train-test-resolution-discrepancy-2
FixEfficientNet-B1|2020|0.826|fixing-the-train-test-resolution-discrepancy-2
FixEfficientNet-B0|2020|0.802|fixing-the-train-test-resolution-discrepancy-2
Meta Pseudo Labels |2020|0.9|meta-pseudo-labels
Meta Pseudo Labels |2020|0.8320000000000001|meta-pseudo-labels
KDforAA |2020|0.858|circumventing-outliers-of-autoaugment-with
KDforAA |2020|0.855|circumventing-outliers-of-autoaugment-with
GreedyNAS-A|2020|0.7709999999999999|greedynas-towards-fast-one-shot-nas-with
GreedyNAS-B|2020|0.768|greedynas-towards-fast-one-shot-nas-with
GreedyNAS-C|2020|0.762|greedynas-towards-fast-one-shot-nas-with
TResNet-XL|2020|0.843|tresnet-high-performance-gpu-dedicated
RegNetY-8.0GF|2020|0.799|designing-network-design-spaces
RegNetY-4.0GF|2020|0.794|designing-network-design-spaces
RegNetY-1.6GF|2020|0.78|designing-network-design-spaces
RegNetY-800MF|2020|0.763|designing-network-design-spaces
RegNetY-600MF|2020|0.755|designing-network-design-spaces
RegNetY-400MF|2020|0.741|designing-network-design-spaces
MUXNet-l|2020|0.7659999999999999|muxconv-information-multiplexing-in
MUXNet-m|2020|0.753|muxconv-information-multiplexing-in
MUXNet-s|2020|0.716|muxconv-information-multiplexing-in
MUXNet-xs|2020|0.667|muxconv-information-multiplexing-in
ResNeSt-269|2020|0.845|resnest-split-attention-networks
ResNeSt-200|2020|0.8390000000000001|resnest-split-attention-networks
ResNeSt-101|2020|0.83|resnest-split-attention-networks
ResNeSt-50|2020|0.8112999999999999|resnest-split-attention-networks
ResNeSt-50-fast|2020|0.8064|resnest-split-attention-networks
ResNet-200 |2020|0.8079999999999999|supervised-contrastive-learning
NAT-M4|2020|0.805|neural-architecture-transfer
Multiscale DEQ |2020|0.792|multiscale-deep-equilibrium-models
ResNet200_vd_26w_4s_ssld|2020|0.851|semi-supervised-recognition-under-a-noisy-and
Fix_ResNet50_vd_ssld|2020|0.84|semi-supervised-recognition-under-a-noisy-and
ResNet50_vd_ssld|2020|0.83|semi-supervised-recognition-under-a-noisy-and
MobileNetV3_large_x1_0_ssld|2020|0.79|semi-supervised-recognition-under-a-noisy-and
PyConvResNet-101|2020|0.8149|pyramidal-convolution-rethinking
Prodpoly|2020|0.7717|deep-polynomial-neural-networks
PS-KD |2020|0.7924|self-knowledge-distillation-a-simple-way-for
ReXNet-R_3.0|2020|0.845|rexnet-diminishing-representational
ReXNet-R_2.0|2020|0.8320000000000001|rexnet-diminishing-representational
ReXNet_3.0|2020|0.828|rexnet-diminishing-representational
ReXNet_2.0|2020|0.816|rexnet-diminishing-representational
ReXNet_1.5|2020|0.8029999999999999|rexnet-diminishing-representational
ReXNet_1.3|2020|0.795|rexnet-diminishing-representational
ReXNet_1.0|2020|0.779|rexnet-diminishing-representational
ReXNet_0.9|2020|0.772|rexnet-diminishing-representational
ReXNet_0.6|2020|0.746|rexnet-diminishing-representational
Ours|2020|0.7197|quantnet-learning-to-quantize-by-learning
ResNet-50|2020|0.7876000000000001|puzzle-mix-exploiting-saliency-and-local-1
MEAL V2 |2020|0.8172|meal-v2-boosting-vanilla-resnet-50-to-80-top
MEAL V2 |2020|0.8067|meal-v2-boosting-vanilla-resnet-50-to-80-top
ResNet-18 |2020|0.7319|meal-v2-boosting-vanilla-resnet-50-to-80-top
iAFF-ResNeXt-50-32x4d|2020|0.8022|attentional-feature-fusion
EfficientNet-L2-475 |2020|0.8861|sharpness-aware-minimization-for-efficiently-1
ResNet-152 |2020|0.816|sharpness-aware-minimization-for-efficiently-1
ResNeXt-101 |2020|0.812|shape-texture-debiased-neural-network-1
ViT-H/14|2020|0.8855|an-image-is-worth-16x16-words-transformers-1
ViT-L/16|2020|0.8776|an-image-is-worth-16x16-words-transformers-1
TinyNet |2020|0.794|model-rubik-s-cube-twisting-resolution-depth
TinyNet-A + RA|2020|0.777|model-rubik-s-cube-twisting-resolution-depth
Grafit |2020|0.7959999999999999|grafit-learning-fine-grained-image
ResNet-18 |2020|0.7171|torchdistill-a-modular-configuration-driven
ResNet-18 |2020|0.7156|torchdistill-a-modular-configuration-driven
ResNet-18 |2020|0.7137|torchdistill-a-modular-configuration-driven
ResNet-18 |2020|0.7108|torchdistill-a-modular-configuration-driven
ResNet-18 |2020|0.7093|torchdistill-a-modular-configuration-driven
ResNet-18 |2020|0.7051999999999999|torchdistill-a-modular-configuration-driven
ResNet-18 |2020|0.7009000000000001|torchdistill-a-modular-configuration-driven
SE-ResNeXt-10|2020|0.836|SE-ResNeXt-10
SE-ResNeXt-10|2020|0.8334|SE-ResNeXt-10
ResNeXt-10|2020|0.8212999999999999|ResNeXt-10
DeiT-B 384|2020|0.852|training-data-efficient-image-transformers
DeiT-B|2020|0.8420000000000001|training-data-efficient-image-transformers
DeiT-B|2020|0.826|training-data-efficient-image-transformers
DeiT-B|2020|0.7659999999999999|training-data-efficient-image-transformers
ResNet-50+AutoDropout+RandAugment|2021|0.8029999999999999|autodropout-learning-dropout-patterns-to
ResNet-50|2021|0.787|autodropout-learning-dropout-patterns-to
EfficientNet-B0|2021|0.775|autodropout-learning-dropout-patterns-to
SSAL-Resnet50|2021|0.77|contextual-classification-using-self
RepVGG-B2|2021|0.7878000000000001|repvgg-making-vgg-style-convnets-great-again
RepVGG-B2g4|2021|0.785|repvgg-making-vgg-style-convnets-great-again
ReXNet_1.0-relabel|2021|0.784|re-labeling-imagenet-from-single-to-multi
BoTNet T7|2021|0.847|bottleneck-transformers-for-visual
BoTNet T7-320|2021|0.8420000000000001|bottleneck-transformers-for-visual
BoTNet T6|2021|0.84|bottleneck-transformers-for-visual
SENet-350|2021|0.838|bottleneck-transformers-for-visual
BoTNet T5|2021|0.835|bottleneck-transformers-for-visual
BoTNet T4|2021|0.828|bottleneck-transformers-for-visual
SENet-152|2021|0.8220000000000001|bottleneck-transformers-for-visual
BoTNet T3|2021|0.8170000000000001|bottleneck-transformers-for-visual
SENet-101|2021|0.8140000000000001|bottleneck-transformers-for-visual
ResNet-101|2021|0.8|bottleneck-transformers-for-visual
SENet-50|2021|0.794|bottleneck-transformers-for-visual
ResNet-50|2021|0.7879999999999999|bottleneck-transformers-for-visual
"T2T-ViT-14|384"|2021|0.833|tokens-to-token-vit-training-vision
T2T-ViTt-24|2021|0.826|tokens-to-token-vit-training-vision
T2T-ViT-24|2021|0.823|tokens-to-token-vit-training-vision
T2T-ViTt-19|2021|0.8220000000000001|tokens-to-token-vit-training-vision
T2T-ViT-19|2021|0.8190000000000001|tokens-to-token-vit-training-vision
T2T-ViT-14|2021|0.815|tokens-to-token-vit-training-vision
ZenNAS |2021|0.83|zen-nas-a-zero-shot-nas-for-high-performance
ZenNet-400M-SE|2021|0.78|zen-nas-a-zero-shot-nas-for-high-performance
NFNet-F4+|2021|0.892|high-performance-large-scale-image
ALIGN |2021|0.8864|scaling-up-visual-and-vision-language
NFNet-F6 w/ SAM|2021|0.865|high-performance-large-scale-image
NFNet-F5 w/ SAM|2021|0.863|high-performance-large-scale-image
NFNet-F5|2021|0.86|high-performance-large-scale-image
NFNet-F4|2021|0.8590000000000001|high-performance-large-scale-image
NFNet-F3|2021|0.857|high-performance-large-scale-image
NFNet-F2|2021|0.851|high-performance-large-scale-image
NFNet-F1|2021|0.847|high-performance-large-scale-image
NFNet-F0|2021|0.836|high-performance-large-scale-image
ResNet-50 MLPerf v0.7 - 2512 steps|2021|0.7592|a-large-batch-optimizer-reality-check
AlphaNet-A6|2021|0.8079999999999999|alphanet-improved-training-of-supernet-with
AlphaNet-A5|2021|0.8029999999999999|alphanet-improved-training-of-supernet-with
AlphaNet-A4|2021|0.8|alphanet-improved-training-of-supernet-with
AlphaNet-A3|2021|0.794|alphanet-improved-training-of-supernet-with
AlphaNet-A2|2021|0.7909999999999999|alphanet-improved-training-of-supernet-with
AlphaNet-A1|2021|0.789|alphanet-improved-training-of-supernet-with
AlphaNet-A0|2021|0.778|alphanet-improved-training-of-supernet-with
LambdaResNet200|2021|0.843|lambdanetworks-modeling-long-range-1
LambdaResNet152|2021|0.84|lambdanetworks-modeling-long-range-1
TNT-B|2021|0.8390000000000001|transformer-in-transformer
Perceiver |2021|0.78|perceiver-general-perception-with-iterative
Perceiver|2021|0.764|perceiver-general-perception-with-iterative
RedNet-152|2021|0.7929999999999999|involution-inverting-the-inherence-of
RedNet-101|2021|0.7909999999999999|involution-inverting-the-inherence-of
RedNet-50|2021|0.784|involution-inverting-the-inherence-of
RedNet-38|2021|0.7759999999999999|involution-inverting-the-inherence-of
RedNet-26|2021|0.759|involution-inverting-the-inherence-of
ResNet-RS-50 |2021|0.8440000000000001|revisiting-resnets-improved-training-and
ResNet-RS-270 |2021|0.838|revisiting-resnets-improved-training-and
ConViT-B+|2021|0.825|convit-improving-vision-transformers-with
ConViT-B|2021|0.8240000000000001|convit-improving-vision-transformers-with
ConViT-S+|2021|0.8220000000000001|convit-improving-vision-transformers-with
ConViT-S|2021|0.813|convit-improving-vision-transformers-with
HVT-S-1|2021|0.78|scalable-visual-transformers-with
ConViT-Ti+|2021|0.767|convit-improving-vision-transformers-with
ConViT-Ti|2021|0.731|convit-improving-vision-transformers-with
HVT-Ti-1|2021|0.6964|scalable-visual-transformers-with
CeiT-S |2021|0.833|incorporating-convolution-designs-into-visual
DeepVit-L* |2021|0.831|deepvit-towards-deeper-vision-transformer
DeepVit-L|2021|0.8220000000000001|deepvit-towards-deeper-vision-transformer
CeiT-S|2021|0.82|incorporating-convolution-designs-into-visual
CeiT-T |2021|0.7879999999999999|incorporating-convolution-designs-into-visual
CeiT-T|2021|0.764|incorporating-convolution-designs-into-visual
BossNet-T1|2021|0.8220000000000001|bossnas-exploring-hybrid-cnn-transformers
ResNet-101 |2021|0.8098000000000001|automix-unveiling-the-power-of-mixup
ResNet-50 |2021|0.7925|automix-unveiling-the-power-of-mixup
ResNet-34 |2021|0.7609999999999999|automix-unveiling-the-power-of-mixup
ResNet-18 |2021|0.7204999999999999|automix-unveiling-the-power-of-mixup
Swin-T|2021|0.813|swin-transformer-hierarchical-vision
CrossViT-18+|2021|0.828|2103-14899
CrossViT-18|2021|0.825|2103-14899
CrossViT-15+|2021|0.823|2103-14899
CrossViT-15|2021|0.815|2103-14899
CvT-21 |2021|0.833|cvt-introducing-convolutions-to-vision
ViL-Medium-D|2021|0.833|2103-15358
ViL-Base-D|2021|0.8320000000000001|2103-15358
CvT-13 |2021|0.83|cvt-introducing-convolutions-to-vision
ViL-Medium-W|2021|0.8290000000000001|2103-15358
CvT-21|2021|0.825|cvt-introducing-convolutions-to-vision
CvT-13-NAS|2021|0.8220000000000001|cvt-introducing-convolutions-to-vision
ViL-Small|2021|0.82|2103-15358
ViL-Base-W|2021|0.8190000000000001|2103-15358
CvT-13|2021|0.816|cvt-introducing-convolutions-to-vision
ViL-Tiny-RPB|2021|0.767|2103-15358
PiT-B|2021|0.84|rethinking-spatial-dimensions-of-vision
PiT-S|2021|0.8190000000000001|rethinking-spatial-dimensions-of-vision
PiT-XS|2021|0.7909999999999999|rethinking-spatial-dimensions-of-vision
PiT-Ti|2021|0.746|rethinking-spatial-dimensions-of-vision
CaiT-M-48-448|2021|0.865|going-deeper-with-image-transformers
CAIT-M36-448|2021|0.863|going-deeper-with-image-transformers
CAIT-M-36|2021|0.861|going-deeper-with-image-transformers
CAIT-M-24|2021|0.858|going-deeper-with-image-transformers
CAIT-S-36|2021|0.8540000000000001|going-deeper-with-image-transformers
CAIT-S-48|2021|0.853|going-deeper-with-image-transformers
CAIT-S-24|2021|0.851|going-deeper-with-image-transformers
CAIT-XS-36|2021|0.848|going-deeper-with-image-transformers
CAIT-XS-24|2021|0.841|going-deeper-with-image-transformers
CAIT-XXS-36|2021|0.8220000000000001|going-deeper-with-image-transformers
CAIT-XXS-24|2021|0.809|going-deeper-with-image-transformers
EfficientNetV2-L |2021|0.868|efficientnetv2-smaller-models-and-faster
EfficientNetV2-M |2021|0.861|efficientnetv2-smaller-models-and-faster
EfficientNetV2-L|2021|0.857|efficientnetv2-smaller-models-and-faster
EfficientNetV2-M|2021|0.851|efficientnetv2-smaller-models-and-faster
EfficientNetV2-S |2021|0.85|efficientnetv2-smaller-models-and-faster
EfficientNetV2-S|2021|0.8390000000000001|efficientnetv2-smaller-models-and-faster
LeViT-384|2021|0.825|levit-a-vision-transformer-in-convnet-s
LeViT-256|2021|0.816|levit-a-vision-transformer-in-convnet-s
LeViT-192|2021|0.8|levit-a-vision-transformer-in-convnet-s
LeViT-128|2021|0.7959999999999999|levit-a-vision-transformer-in-convnet-s
LeViT-128S|2021|0.757|levit-a-vision-transformer-in-convnet-s
"CCT-14/7x2 | 384"|2021|0.8271|escaping-the-big-data-paradigm-with-compact
CCT-14/7x2|2021|0.8134|escaping-the-big-data-paradigm-with-compact
LocalViT-S|2021|0.8079999999999999|localvit-bringing-locality-to-vision
CCT-16/7x2|2021|0.8028|escaping-the-big-data-paradigm-with-compact
LocalViT-PVT|2021|0.782|localvit-bringing-locality-to-vision
LocalViT-TNT|2021|0.759|localvit-bringing-locality-to-vision
LocalViT-T|2021|0.748|localvit-bringing-locality-to-vision
LocalViT-T2T|2021|0.725|localvit-bringing-locality-to-vision
AsymmNet-Large \u00d71.0|2021|0.754|asymmnet-towards-ultralight-convolution
AsymmNet-Large \u00d70.5|2021|0.6920000000000001|asymmnet-towards-ultralight-convolution
AsymmNet-Small \u00d71.0|2021|0.684|asymmnet-towards-ultralight-convolution
PDC|2021|0.716|polynomial-networks-in-deep-classifiers
ReActNet-A |2021|0.68|bnn-bn-training-binary-neural-networks
DIFFQ |2021|0.82|differentiable-model-compression-via-pseudo
LV-ViT-L|2021|0.8640000000000001|token-labeling-training-a-85-5-top-1-accuracy
MViT-B-24|2021|0.848|multiscale-vision-transformers
LV-ViT-M|2021|0.841|token-labeling-training-a-85-5-top-1-accuracy
LV-ViT-S|2021|0.833|token-labeling-training-a-85-5-top-1-accuracy
MViT-B-16|2021|0.83|multiscale-vision-transformers
Visformer-S|2021|0.8220000000000001|visformer-the-vision-friendly-transformer
Visformer-Ti|2021|0.7859999999999999|visformer-the-vision-friendly-transformer
Twins-SVT-L|2021|0.8370000000000001|twins-revisiting-spatial-attention-design-in
Mixer-H/14 |2021|0.8794|mlp-mixer-an-all-mlp-architecture-for-vision
ViT-L/16 Dosovitskiy et al. |2021|0.853|mlp-mixer-an-all-mlp-architecture-for-vision
Mixer-B/16|2021|0.7644|mlp-mixer-an-all-mlp-architecture-for-vision
T2T-ViT-14|2021|0.8170000000000001|beyond-self-attention-external-attention
RepMLP-Res50|2021|0.7859999999999999|repmlp-re-parameterizing-convolutions-into
FF|2021|0.7490000000000001|do-you-even-need-attention-a-stack-of-feed
ResMLP-B24/8|2021|0.836|resmlp-feedforward-networks-for-image
ResMLP-S24|2021|0.8079999999999999|resmlp-feedforward-networks-for-image
BasisNet-MV3|2021|0.8|basisnet-two-stage-model-synthesis-for-1
ResMLP-36|2021|0.797|resmlp-feedforward-networks-for-image
ResMLP-24|2021|0.794|resmlp-feedforward-networks-for-image
ResMLP-S12|2021|0.778|resmlp-feedforward-networks-for-image
Conformer-B|2021|0.841|conformer-local-features-coupling-global
RVT-B*|2021|0.8270000000000001|rethinking-the-design-principles-of-robust
RVT-S*|2021|0.8190000000000001|rethinking-the-design-principles-of-robust
gMLP-B|2021|0.816|pay-attention-to-mlps
RVT-Ti*|2021|0.792|rethinking-the-design-principles-of-robust
Heteroscedastic |2021|0.6859999999999999|correlated-input-dependent-label-noise-in
Transformer local-attention |2021|0.838|aggregating-nested-transformers
Transformer local-attention |2021|0.833|aggregating-nested-transformers
Transformer local-attention |2021|0.815|aggregating-nested-transformers
NFNet-F5 w/ SAM w/ augmult=16|2021|0.8678|drawing-multiple-augmentation-samples-per
ResT-Large|2021|0.836|rest-an-efficient-transformer-for-visual
ResT-Small|2021|0.7959999999999999|rest-an-efficient-transformer-for-visual
DVT |2021|0.8043|not-all-images-are-worth-16x16-words-dynamic
DVT |2021|0.7974|not-all-images-are-worth-16x16-words-dynamic
DVT |2021|0.7848|not-all-images-are-worth-16x16-words-dynamic
Container Container|2021|0.8270000000000001|container-context-aggregation-network
Container-Light|2021|0.82|container-context-aggregation-network
DynamicViT-LV-M/0.8|2021|0.8390000000000001|dynamicvit-efficient-vision-transformers-with
ResNet-152x2-SAM|2021|0.8109999999999999|when-vision-transformers-outperform-resnets
ViT-B/16-SAM|2021|0.799|when-vision-transformers-outperform-resnets
Mixer-B/8-SAM|2021|0.79|when-vision-transformers-outperform-resnets
Refiner-ViT-L|2021|0.8603000000000001|refiner-refining-self-attention-for-vision
ViTAE-B-Stage|2021|0.836|vitae-vision-transformer-advanced-by
ViTAE-S-Stage|2021|0.8220000000000001|vitae-vision-transformer-advanced-by
ViTAE-13M|2021|0.81|vitae-vision-transformer-advanced-by
ViTAE-6M|2021|0.779|vitae-vision-transformer-advanced-by
ViTAE-T-Stage|2021|0.768|vitae-vision-transformer-advanced-by
ViTAE-T|2021|0.753|vitae-vision-transformer-advanced-by
CoAtNet-6|2021|0.9045000000000001|coatnet-marrying-convolution-and-attention
CoAtNet-3 @384|2021|0.8852|coatnet-marrying-convolution-and-attention
FunMatch - T384+224 |2021|0.828|knowledge-distillation-a-good-teacher-is
V-MoE-H/14 |2021|0.8835999999999999|scaling-vision-with-sparse-mixture-of-experts
V-MoE-H/14 |2021|0.8823000000000001|scaling-vision-with-sparse-mixture-of-experts
VIT-H/14|2021|0.8808|scaling-vision-with-sparse-mixture-of-experts
V-MoE-L/16 |2021|0.8741|scaling-vision-with-sparse-mixture-of-experts
BEiT-L |2021|0.8859999999999999|beit-bert-pre-training-of-image-transformers
BEiT-L |2021|0.863|beit-bert-pre-training-of-image-transformers
XCiT-L24|2021|0.86|xcit-cross-covariance-image-transformers
XCiT-M24|2021|0.858|xcit-cross-covariance-image-transformers
XCiT-S24|2021|0.856|xcit-cross-covariance-image-transformers
XCiT-S12|2021|0.851|xcit-cross-covariance-image-transformers
TokenLearner L/8 |2021|0.8887|tokenlearner-what-can-8-learned-tokens-do-for
16-TokenLearner B/16 |2021|0.8706999999999999|tokenlearner-what-can-8-learned-tokens-do-for
VOLO-D5|2021|0.871|volo-vision-outlooker-for-visual-recognition
VOLO-D4|2021|0.868|volo-vision-outlooker-for-visual-recognition
VOLO-D3|2021|0.863|volo-vision-outlooker-for-visual-recognition
VOLO-D2|2021|0.86|volo-vision-outlooker-for-visual-recognition
VOLO-D1|2021|0.852|volo-vision-outlooker-for-visual-recognition
PVTv2-B4|2021|0.838|pvtv2-improved-baselines-with-pyramid-vision
PVTv2-B3|2021|0.8320000000000001|pvtv2-improved-baselines-with-pyramid-vision
PVTv2-B2|2021|0.82|pvtv2-improved-baselines-with-pyramid-vision
PVTv2-B1|2021|0.787|pvtv2-improved-baselines-with-pyramid-vision
PVTv2-B0|2021|0.705|pvtv2-improved-baselines-with-pyramid-vision
CSWin-L |2021|0.875|cswin-transformer-a-general-vision
GFNet-H-B|2021|0.8290000000000001|global-filter-networks-for-image
AutoFormer-base|2021|0.8240000000000001|autoformer-searching-transformers-for-visual
AutoFormer-small|2021|0.8170000000000001|autoformer-searching-transformers-for-visual
AutoFormer-tiny|2021|0.747|autoformer-searching-transformers-for-visual
GLiT-Bases|2021|0.823|glit-neural-architecture-search-for-global
GLiT-Smalls|2021|0.805|glit-neural-architecture-search-for-global
GLiT-Tinys|2021|0.763|glit-neural-architecture-search-for-global
CoE-Large + CondConv|2021|0.815|collaboration-of-experts-achieving-80-top-1
CoE-Large|2021|0.807|collaboration-of-experts-achieving-80-top-1
CoE-Small + CondConv + PWLU|2021|0.8|collaboration-of-experts-achieving-80-top-1
"ViP-B|384"|2021|0.8420000000000001|visual-parser-representing-part-whole
CycleMLP-B5|2021|0.8320000000000001|cyclemlp-a-mlp-like-architecture-for-dense
SkipblockNet-L|2021|0.7709999999999999|bias-loss-for-mobile-neural-networks
SkipblockNet-M|2021|0.762|bias-loss-for-mobile-neural-networks
WideNet-H|2021|0.8009000000000001|go-wider-instead-of-deeper
WideNet-L|2021|0.7948999999999999|go-wider-instead-of-deeper
WideNet-B|2021|0.7754000000000001|go-wider-instead-of-deeper
SE-CoTNetD-152|2021|0.846|contextual-transformer-networks-for-visual
SE-CoTNetD-101|2021|0.8320000000000001|contextual-transformer-networks-for-visual
ResNet-200|2021|0.818|parametric-contrastive-learning
SE-CoTNetD-50|2021|0.816|contextual-transformer-networks-for-visual
ResNet-152|2021|0.813|parametric-contrastive-learning
ResNet-101|2021|0.809|parametric-contrastive-learning
DeiT-B with iRPE-K|2021|0.8240000000000001|rethinking-and-improving-relative-position
DeiT-S with iRPE-QKV|2021|0.8140000000000001|rethinking-and-improving-relative-position
DeiT-S with iRPE-QK|2021|0.8109999999999999|rethinking-and-improving-relative-position
DeiT-S with iRPE-K|2021|0.809|rethinking-and-improving-relative-position
DeiT-Ti with iRPE-K|2021|0.737|rethinking-and-improving-relative-position
Evo-LeViT-384*|2021|0.8220000000000001|evo-vit-slow-fast-token-evolution-for-dynamic
Co-ResNet-152|2021|0.7903|contextual-convolutional-neural-networks
ConvMLP-L|2021|0.802|convmlp-hierarchical-convolutional-mlps-for
ConvMLP-M|2021|0.79|convmlp-hierarchical-convolutional-mlps-for
ConvMLP-S|2021|0.768|convmlp-hierarchical-convolutional-mlps-for
sMLPNet-B |2021|0.8340000000000001|sparse-mlp-for-image-recognition-is-self
sMLPNet-S |2021|0.831|sparse-mlp-for-image-recognition-is-self
sMLPNet-T |2021|0.8190000000000001|sparse-mlp-for-image-recognition-is-self
NASViT |2021|0.8290000000000001|nasvit-neural-architecture-search-for
ConvMixer-1536/20|2021|0.8220000000000001|patches-are-all-you-need-1
NASViT-A5|2021|0.818|nasvit-neural-architecture-search-for
NASViT-A4|2021|0.8140000000000001|nasvit-neural-architecture-search-for
NASViT-A3|2021|0.81|nasvit-neural-architecture-search-for
NASViT-A2|2021|0.805|nasvit-neural-architecture-search-for
DAFT-conv |2021|0.802|a-dot-product-attention-free-transformer
DAFT-full|2021|0.7979999999999999|a-dot-product-attention-free-transformer
NASViT-A1|2021|0.797|nasvit-neural-architecture-search-for
NASViT-A0|2021|0.782|nasvit-neural-architecture-search-for
ResNet-152 |2021|0.8240000000000001|resnet-strikes-back-an-improved-training
ResNet-152 |2021|0.818|resnet-strikes-back-an-improved-training
DeiT-S |2021|0.804|resnet-strikes-back-an-improved-training
ResNet50 |2021|0.804|resnet-strikes-back-an-improved-training
ResNet50 |2021|0.7809999999999999|resnet-strikes-back-an-improved-training
MobileViT-S|2021|0.784|mobilevit-light-weight-general-purpose-and
MobileViT-XS|2021|0.748|mobilevit-light-weight-general-purpose-and
UniNet-B5|2021|0.852|uninet-unified-architecture-search-with
UniNet-B4|2021|0.8420000000000001|uninet-unified-architecture-search-with
UniNet-B2|2021|0.8270000000000001|uninet-unified-architecture-search-with
UniNet-B1|2021|0.804|uninet-unified-architecture-search-with
UniNet-B0|2021|0.7909999999999999|uninet-unified-architecture-search-with
HRFormer-B|2021|0.828|hrformer-high-resolution-transformer-for
HRFormer-T|2021|0.785|hrformer-high-resolution-transformer-for
SReT-T|2021|0.7759999999999999|sliced-recursive-transformer-1
SReT-ExT|2021|0.74|sliced-recursive-transformer-1
MAE |2021|0.8690000000000001|masked-autoencoders-are-scalable-vision
MAE |2021|0.8590000000000001|masked-autoencoders-are-scalable-vision
MAE |2021|0.836|masked-autoencoders-are-scalable-vision
SwinV2-G|2021|0.9017000000000001|swin-transformer-v2-scaling-up-capacity-and
SwinV2-B|2021|0.871|swin-transformer-v2-scaling-up-capacity-and
FBNetV5-F-CLS|2021|0.841|fbnetv5-neural-architecture-search-for
FBNetV5-C-CLS|2021|0.826|fbnetv5-neural-architecture-search-for
FBNetV5|2021|0.818|fbnetv5-neural-architecture-search-for
FBNetV5-A-CLS|2021|0.8170000000000001|fbnetv5-neural-architecture-search-for
FBNetV5-AC-CLS|2021|0.784|fbnetv5-neural-architecture-search-for
FBNetV5-AR-CLS|2021|0.772|fbnetv5-neural-architecture-search-for
DiscreteViT|2021|0.8506999999999999|discrete-representations-strengthen-vision-1
Florence-CoSwin-H|2021|0.9005|florence-a-new-foundation-model-for-computer
MetaFormer PoolFormer-M48|2021|0.825|metaformer-is-actually-what-you-need-for
ResNet-101 |2021|0.8108|boosting-discriminative-visual-representation
ResNet-50 |2021|0.7940999999999999|boosting-discriminative-visual-representation
ResNet-34 |2021|0.7635|boosting-discriminative-visual-representation
ResNet-18 |2021|0.7232999999999999|boosting-discriminative-visual-representation
Dspike |2021|0.7123999999999999|differentiable-spike-rethinking-gradient
MViTv2-H |2021|0.88|improved-multiscale-vision-transformers-for
MViTv2-L |2021|0.863|improved-multiscale-vision-transformers-for
MViTv2-T|2021|0.823|improved-multiscale-vision-transformers-for
SReT-LT |2021|0.787|a-fast-knowledge-distillation-framework-for
QnA-ViT-Base|2021|0.8370000000000001|learned-queries-for-efficient-local-attention
QnA-ViT-Small|2021|0.8320000000000001|learned-queries-for-efficient-local-attention
RepMLPNet-L256|2021|0.818|repmlpnet-hierarchical-vision-mlp-with-re
QnA-ViT-Tiny|2021|0.8170000000000001|learned-queries-for-efficient-local-attention
ELSA-VOLO-D5 |2021|0.872|elsa-enhanced-local-self-attention-for-vision
ELSA-VOLO-D1|2021|0.847|elsa-enhanced-local-self-attention-for-vision
ELSA-Swin-T|2021|0.8270000000000001|elsa-enhanced-local-self-attention-for-vision
PatchConvNet-L120-21k-384|2021|0.871|augmenting-convolutional-networks-with
PatchConvNet-B60-21k-384|2021|0.865|augmenting-convolutional-networks-with
PatchConvNet-S60-21k-512|2021|0.8540000000000001|augmenting-convolutional-networks-with
PatchConvNet-B120|2021|0.841|augmenting-convolutional-networks-with
PatchConvNet-B60|2021|0.835|augmenting-convolutional-networks-with
PatchConvNet-S120|2021|0.8320000000000001|augmenting-convolutional-networks-with
PatchConvNet-S60|2021|0.821|augmenting-convolutional-networks-with
DAT-S|2022|0.8370000000000001|vision-transformer-with-deformable-attention
DAT-T|2022|0.82|vision-transformer-with-deformable-attention
ConvNeXt-XL |2022|0.878|a-convnet-for-the-2020s
ConvNeXt-L |2022|0.855|a-convnet-for-the-2020s
ConvNeXt-T|2022|0.821|a-convnet-for-the-2020s
SWAG |2022|0.8859999999999999|revisiting-weakly-supervised-pre-training-of
Omnivore |2022|0.86|omnivore-a-single-model-for-many-visual
Omnivore |2022|0.853|omnivore-a-single-model-for-many-visual
UniFormer-L |2022|0.863|uniformer-unifying-convolution-and-self
UniFormer-L|2022|0.856|uniformer-unifying-convolution-and-self
UniFormer-S|2022|0.8340000000000001|uniformer-unifying-convolution-and-self
Shift-B|2022|0.833|when-shift-operation-meets-vision-transformer
Shift-S|2022|0.828|when-shift-operation-meets-vision-transformer
Shift-T|2022|0.8170000000000001|when-shift-operation-meets-vision-transformer
data2vec |2022|0.866|data2vec-a-general-framework-for-self-1
MKD ViT-L|2022|0.865|meta-knowledge-distillation
SEER |2022|0.858|vision-models-are-more-robust-and-fair-when
MKD ViT-B|2022|0.851|meta-knowledge-distillation
MKD ViT-S|2022|0.831|meta-knowledge-distillation
MKD ViT-T|2022|0.7709999999999999|meta-knowledge-distillation
VAN-B5 |2022|0.863|visual-attention-network
VAN-B4 |2022|0.857|visual-attention-network
VAN-B2|2022|0.828|visual-attention-network
VAN-B1|2022|0.8109999999999999|visual-attention-network
VAN-B0|2022|0.754|visual-attention-network
ViTAE-H + MAE |2022|0.885|vitaev2-vision-transformer-advanced-by
EdgeFormer-S|2022|0.7863|edgeformer-improving-light-weight-convnets-by
Model soups |2022|0.9094|model-soups-averaging-weights-of-multiple
ActiveMLP-L|2022|0.848|activemlp-an-mlp-like-architecture-with
ActiveMLP-T|2022|0.82|activemlp-an-mlp-like-architecture-with
RepLKNet-XL|2022|0.878|scaling-up-your-kernels-to-31x31-revisiting
ViT-L@384 |2022|0.855|three-things-everyone-should-know-about
ViT-B@384 |2022|0.843|three-things-everyone-should-know-about
ViT-B-36x1|2022|0.841|three-things-everyone-should-know-about
ViT-B-18x2|2022|0.841|three-things-everyone-should-know-about
ViT-B |2022|0.8340000000000001|three-things-everyone-should-know-about
ViT-S-24x2|2022|0.826|three-things-everyone-should-know-about
ViT-S-48x1|2022|0.823|three-things-everyone-should-know-about
ELP |2022|0.7613|a-simple-episodic-linear-probe-improves
VOLO-D5+HAT|2022|0.873|improving-vision-transformers-by-revisiting
kNN-CLIP|2022|0.7979999999999999|revisiting-a-knn-based-image-classification
MaxViT-L |2022|0.867|maxvit-multi-axis-vision-transformer
MaxViT-L |2022|0.8640000000000001|maxvit-multi-axis-vision-transformer
MaxViT-B |2022|0.8634000000000001|maxvit-multi-axis-vision-transformer
MaxViT-S |2022|0.8619|maxvit-multi-axis-vision-transformer
MaxViT-T|2022|0.8572|maxvit-multi-axis-vision-transformer
MaxViT-T |2022|0.8523999999999999|maxvit-multi-axis-vision-transformer
MaxViT-B |2022|0.8495|maxvit-multi-axis-vision-transformer
MaxViT-S |2022|0.8445|maxvit-multi-axis-vision-transformer
MaxViT-T |2022|0.8362|maxvit-multi-axis-vision-transformer
DaViT-G|2022|0.904|davit-dual-attention-vision-transformers
DaViT-H|2022|0.902|davit-dual-attention-vision-transformers
DaViT-L |2022|0.875|davit-dual-attention-vision-transformers
DaViT-B |2022|0.8690000000000001|davit-dual-attention-vision-transformers
DaViT-B|2022|0.846|davit-dual-attention-vision-transformers
DaViT-T|2022|0.828|davit-dual-attention-vision-transformers
ViT-L @384 |2022|0.858|deit-iii-revenge-of-the-vit
Mini-Swin-B@384|2022|0.855|minivit-compressing-vision-transformers-with
ViT-H @224 |2022|0.852|deit-iii-revenge-of-the-vit
ViT-B @384 |2022|0.85|deit-iii-revenge-of-the-vit
ViT-L @224 |2022|0.8490000000000001|deit-iii-revenge-of-the-vit
NAT-Base|2022|0.843|neighborhood-attention-transformer
ViT-B @224 |2022|0.838|deit-iii-revenge-of-the-vit
NAT-Small|2022|0.8370000000000001|neighborhood-attention-transformer
ViT-S @384 |2022|0.8340000000000001|deit-iii-revenge-of-the-vit
NAT-Tiny|2022|0.8320000000000001|neighborhood-attention-transformer
NAT-Mini|2022|0.818|neighborhood-attention-transformer
ViT-S @224 |2022|0.8140000000000001|deit-iii-revenge-of-the-vit
EfficientNetV2 |2022|0.872|polyloss-a-polynomial-expansion-perspective-1
FAN-L-Hybrid++|2022|0.871|understanding-the-robustness-in-vision
ASF-former-B|2022|0.8390000000000001|adaptive-split-fusion-transformer
ASF-former-S|2022|0.8270000000000001|adaptive-split-fusion-transformer
CoCa |2022|0.9059999999999999|coca-contrastive-captioners-are-image-text
Sequencer2D-L\u2191392|2022|0.846|sequencer-deep-lstm-for-image-classification
Sequencer2D-L|2022|0.8340000000000001|sequencer-deep-lstm-for-image-classification
Sequencer2D-M|2022|0.828|sequencer-deep-lstm-for-image-classification
Sequencer2D-S|2022|0.823|sequencer-deep-lstm-for-image-classification
CLCNet |2022|0.8661|clcnet-rethinking-of-ensemble-modeling-with
CLCNet |2022|0.8645999999999999|clcnet-rethinking-of-ensemble-modeling-with
CLCNet |2022|0.8642|clcnet-rethinking-of-ensemble-modeling-with
CLCNet |2022|0.8528|clcnet-rethinking-of-ensemble-modeling-with
CLCNet |2022|0.8388|clcnet-rethinking-of-ensemble-modeling-with
Bamboo |2022|0.871|deeper-vs-wider-a-revisit-of-transformer
Bamboo |2022|0.863|deeper-vs-wider-a-revisit-of-transformer
Bamboo |2022|0.8420000000000001|deeper-vs-wider-a-revisit-of-transformer
\u00b52Net |2022|0.8674|an-evolutionary-approach-to-dynamic
MixMIM-B|2022|0.851|mixmim-mixed-and-masked-image-modeling-for
"LITv2-B|384"|2022|0.847|fast-vision-transformers-with-hilo-attention
TransBoost-ViT-S|2022|0.8367|transboost-improving-the-best-imagenet
LITv2-B|2022|0.836|fast-vision-transformers-with-hilo-attention
LITv2-M|2022|0.833|fast-vision-transformers-with-hilo-attention
TransBoost-ConvNext-T|2022|0.8245999999999999|transboost-improving-the-best-imagenet
TransBoost-Swin-T|2022|0.8216|transboost-improving-the-best-imagenet
LITv2-S|2022|0.82|fast-vision-transformers-with-hilo-attention
TransBoost-ResNet50-StrikesBack|2022|0.8115000000000001|transboost-improving-the-best-imagenet
TransBoost-ResNet152|2022|0.8064|transboost-improving-the-best-imagenet
TransBoost-ResNet101|2022|0.7986|transboost-improving-the-best-imagenet
TransBoost-ResNet50|2022|0.7903|transboost-improving-the-best-imagenet
TransBoost-EfficientNetB0|2022|0.7859999999999999|transboost-improving-the-best-imagenet
TransBoost-MobileNetV3-L|2022|0.7681|transboost-improving-the-best-imagenet
TransBoost-ResNet34|2022|0.767|transboost-improving-the-best-imagenet
TransBoost-ResNet18|2022|0.7336|transboost-improving-the-best-imagenet
FD |2022|0.89|contrastive-learning-rivals-masked-image
WaveMix-192/16 |2022|0.7493000000000001|wavemix-lite-a-resource-efficient-neural
EfficientViT-B3 |2022|0.8420000000000001|efficientvit-enhanced-linear-attention-for
Pyramid ViG-B|2022|0.8370000000000001|vision-gnn-an-image-is-worth-graph-of-nodes
Pyramid ViG-M|2022|0.831|vision-gnn-an-image-is-worth-graph-of-nodes
Pyramid ViG-S|2022|0.821|vision-gnn-an-image-is-worth-graph-of-nodes
Pyramid ViG-Ti|2022|0.782|vision-gnn-an-image-is-worth-graph-of-nodes
MobileOne-S4 |2022|0.8140000000000001|an-improved-one-millisecond-mobile-backbone
MobileOne-S3 |2022|0.8|an-improved-one-millisecond-mobile-backbone
MobileOne-S4|2022|0.794|an-improved-one-millisecond-mobile-backbone
MobileOne-S2 |2022|0.7909999999999999|an-improved-one-millisecond-mobile-backbone
MobileOne-S3|2022|0.7809999999999999|an-improved-one-millisecond-mobile-backbone
MobileOne-S2|2022|0.774|an-improved-one-millisecond-mobile-backbone
MobileOne-S1 |2022|0.774|an-improved-one-millisecond-mobile-backbone
MobileOne-S1|2022|0.759|an-improved-one-millisecond-mobile-backbone
MobileOne-S0 |2022|0.725|an-improved-one-millisecond-mobile-backbone
MobileOne-S0|2022|0.7140000000000001|an-improved-one-millisecond-mobile-backbone
Top-k DiffSortNets |2022|0.8837|differentiable-top-k-classification-learning-1
"Our SP-ViT-L|384"|2022|0.863|sp-vit-learning-2d-spatial-priors-for-vision
"Our SP-ViT-M|384"|2022|0.86|sp-vit-learning-2d-spatial-priors-for-vision
Our SP-ViT-L|2022|0.855|sp-vit-learning-2d-spatial-priors-for-vision
"SP-ViT-S|384"|2022|0.851|sp-vit-learning-2d-spatial-priors-for-vision
Our SP-ViT-M|2022|0.8490000000000001|sp-vit-learning-2d-spatial-priors-for-vision
Our SP-ViT-S|2022|0.8390000000000001|sp-vit-learning-2d-spatial-priors-for-vision
GC ViT-B|2022|0.845|global-context-vision-transformers
GC ViT-S|2022|0.84|global-context-vision-transformers
GC ViT-T|2022|0.8340000000000001|global-context-vision-transformers
GC ViT-XT|2022|0.82|global-context-vision-transformers
GC ViT-XXT|2022|0.7979999999999999|global-context-vision-transformers
EdgeNeXt-S|2022|0.794|edgenext-efficiently-amalgamated-cnn
EdgeNeXt-XXS|2022|0.7120000000000001|edgenext-efficiently-amalgamated-cnn
RevBiFPN-S6|2022|0.8420000000000001|revbifpn-the-fully-reversible-bidirectional
RevBiFPN-S5|2022|0.8370000000000001|revbifpn-the-fully-reversible-bidirectional
RevBiFPN-S4|2022|0.83|revbifpn-the-fully-reversible-bidirectional
RevBiFPN-S3|2022|0.8109999999999999|revbifpn-the-fully-reversible-bidirectional
RevBiFPN-S2|2022|0.79|revbifpn-the-fully-reversible-bidirectional
RevBiFPN-S1|2022|0.759|revbifpn-the-fully-reversible-bidirectional
RevBiFPN-S0|2022|0.728|revbifpn-the-fully-reversible-bidirectional
Wave-ViT-L|2022|0.855|wave-vit-unifying-wavelet-and-transformers
Wave-ViT-B|2022|0.848|wave-vit-unifying-wavelet-and-transformers
Wave-ViT-S|2022|0.8390000000000001|wave-vit-unifying-wavelet-and-transformers
UniNet-B6|2022|0.8740000000000001|uninet-unified-architecture-search-with-1
UniNet-B5|2022|0.87|uninet-unified-architecture-search-with-1
Next-ViT-L @384|2022|0.847|next-vit-next-generation-vision-transformer
Next-ViT-B|2022|0.8320000000000001|next-vit-next-generation-vision-transformer
Next-ViT-S|2022|0.825|next-vit-next-generation-vision-transformer
UniNet-B0|2022|0.8079999999999999|uninet-unified-architecture-search-with-1
TinyViT-21M-distill |2022|0.848|tinyvit-fast-pretraining-distillation-for
TinyViT-11M-distill |2022|0.8320000000000001|tinyvit-fast-pretraining-distillation-for
TinyViT-21M|2022|0.831|tinyvit-fast-pretraining-distillation-for
TinyViT-11M|2022|0.815|tinyvit-fast-pretraining-distillation-for
TinyViT-5M-distill |2022|0.807|tinyvit-fast-pretraining-distillation-for
TinyViT-5M|2022|0.7909999999999999|tinyvit-fast-pretraining-distillation-for
ResMLP-B24 + STD|2022|0.8240000000000001|spatial-channel-token-distillation-for-vision
CycleMLP-B2 + STD|2022|0.821|spatial-channel-token-distillation-for-vision
Mixer-S16 + STD|2022|0.757|spatial-channel-token-distillation-for-vision
HorNet-L |2022|0.877|hornet-efficient-high-order-spatial
gSwin-S|2022|0.8301000000000001|gswin-gated-mlp-vision-model-with
gSwin-T|2022|0.8170999999999999|gswin-gated-mlp-vision-model-with
gSwin-VT|2022|0.8031999999999999|gswin-gated-mlp-vision-model-with
dBOT ViT-H |2022|0.882|exploring-target-representations-for-masked
dBOT ViT-L |2022|0.878|exploring-target-representations-for-masked
dBOT ViT-B |2022|0.857|exploring-target-representations-for-masked
ViT-e|2022|0.909|pali-a-jointly-scaled-multilingual-language
MAE+DAT |2022|0.8702|enhance-the-visual-representation-via
Mega|2022|0.8240000000000001|mega-moving-average-equipped-gated-attention
GPaCo |2022|0.8601000000000001|generalized-parametric-contrastive-learning
GPaCo |2022|0.84|generalized-parametric-contrastive-learning
GPaCo |2022|0.797|generalized-parametric-contrastive-learning
DiNAT-Large |2022|0.875|dilated-neighborhood-attention-transformer
DiNAT-Large |2022|0.8740000000000001|dilated-neighborhood-attention-transformer
DiNAT_s-Large |2022|0.8740000000000001|dilated-neighborhood-attention-transformer
DiNAT_s-Large |2022|0.865|dilated-neighborhood-attention-transformer
DiNAT-Base|2022|0.8440000000000001|dilated-neighborhood-attention-transformer
DiNAT-Small|2022|0.838|dilated-neighborhood-attention-transformer
DiNAT-Tiny|2022|0.8270000000000001|dilated-neighborhood-attention-transformer
DiNAT-Mini|2022|0.818|dilated-neighborhood-attention-transformer
MobileViTv3-S|2022|0.7929999999999999|mobilevitv3-mobile-friendly-vision
MobileViTv3-1.0|2022|0.7864|mobilevitv3-mobile-friendly-vision
MobileViTv3-XS|2022|0.767|mobilevitv3-mobile-friendly-vision
MobileViTv3-0.75|2022|0.7655|mobilevitv3-mobile-friendly-vision
MobileViTv3-0.5|2022|0.7232999999999999|mobilevitv3-mobile-friendly-vision
MobileViTv3-XXS|2022|0.7098|mobilevitv3-mobile-friendly-vision
MOAT-4 22K+1K|2022|0.8909999999999999|moat-alternating-mobile-convolution-and
MOAT-3 1K only|2022|0.867|moat-alternating-mobile-convolution-and
MOAT-0 1K only|2022|0.833|moat-alternating-mobile-convolution-and
OTTT|2022|0.6515000000000001|online-training-through-time-for-spiking
WaveMixLite-256/24|2022|0.677|wavemix-lite-a-resource-efficient-neural-1
ViT-B/16-224+HTM|2022|0.8237000000000001|tokenmixup-efficient-attention-guided-token
CAFormer-B36 |2022|0.8640000000000001|metaformer-baselines-for-vision
CAFormer-M36 |2022|0.862|metaformer-baselines-for-vision
CAFormer-S36 |2022|0.857|metaformer-baselines-for-vision
ConvFormer-B36 |2022|0.857|metaformer-baselines-for-vision
ConvFormer-M36 |2022|0.856|metaformer-baselines-for-vision
CAFormer-B36 |2022|0.855|metaformer-baselines-for-vision
ConvFormer-S36 |2022|0.8540000000000001|metaformer-baselines-for-vision
CAFormer-M36 |2022|0.852|metaformer-baselines-for-vision
CAFormer-S18 |2022|0.85|metaformer-baselines-for-vision
ConvFormer-B36 |2022|0.848|metaformer-baselines-for-vision
CAFormer-S36 |2022|0.845|metaformer-baselines-for-vision
ConvFormer-M36 |2022|0.845|metaformer-baselines-for-vision
ConvFormer-S18 |2022|0.8440000000000001|metaformer-baselines-for-vision
ConvFormer-S36 |2022|0.841|metaformer-baselines-for-vision
CAFormer-S18 |2022|0.836|metaformer-baselines-for-vision
ConvFormer-S18 |2022|0.83|metaformer-baselines-for-vision
MogaNet-XL |2022|0.878|efficient-multi-order-gated-aggregation
MogaNet-L|2022|0.847|efficient-multi-order-gated-aggregation
MogaNet-B|2022|0.843|efficient-multi-order-gated-aggregation
MogaNet-S|2022|0.8340000000000001|efficient-multi-order-gated-aggregation
MogaNet-T |2022|0.8|efficient-multi-order-gated-aggregation
MogaNet-XT |2022|0.772|efficient-multi-order-gated-aggregation
InternImage-DCNv3-G |2022|0.9009999999999999|internimage-exploring-large-scale-vision
InternImage-H|2022|0.8959999999999999|internimage-exploring-large-scale-vision
InternImage-XL|2022|0.88|internimage-exploring-large-scale-vision
InternImage-L|2022|0.877|internimage-exploring-large-scale-vision
InternImage-B|2022|0.8490000000000001|internimage-exploring-large-scale-vision
InternImage-S|2022|0.8420000000000001|internimage-exploring-large-scale-vision
InternImage-T|2022|0.835|internimage-exploring-large-scale-vision
EVA|2022|0.897|eva-exploring-the-limits-of-masked-visual
M3I Pre-training |2022|0.8959999999999999|towards-all-in-one-pre-training-via
Heinsen Routing + BEiT-large 16 224|2022|0.867|an-algorithm-for-routing-vectors-in-sequences
Last Layer Tuning with Newton Step |2022|0.889|differentially-private-image-classification
SALG-ST|2022|0.759|semantic-aware-local-global-vision
PAT-B|2022|0.836|pattern-attention-transformer-with-doughnut
PAT-S|2022|0.831|pattern-attention-transformer-with-doughnut
IPT-B|2022|0.836|incepformer-efficient-inception-transformer
IPT-S|2022|0.8290000000000001|incepformer-efficient-inception-transformer
IPT-T|2022|0.805|incepformer-efficient-inception-transformer
ViT-H@224 |2022|0.88|co-training-2-l-submodels-for-visual
ViT-L@224 |2022|0.875|co-training-2-l-submodels-for-visual
Swin-L@224 |2022|0.871|co-training-2-l-submodels-for-visual
ViT-B@224 |2022|0.863|co-training-2-l-submodels-for-visual
Swin-B@224 |2022|0.862|co-training-2-l-submodels-for-visual
ConvNeXt-B@224 |2022|0.858|co-training-2-l-submodels-for-visual
PiT-B@224 |2022|0.858|co-training-2-l-submodels-for-visual
ViT-M@224 |2022|0.85|co-training-2-l-submodels-for-visual
RegnetY16GF@224 |2022|0.8420000000000001|co-training-2-l-submodels-for-visual
ViT-S@224 |2022|0.831|co-training-2-l-submodels-for-visual
R-Mix |2022|0.7739|expeditious-saliency-guided-mix-up-through
OpenCLIP ViT-H/14|2022|0.885|reproducible-scaling-laws-for-contrastive
data2vec 2.0|2022|0.8740000000000001|efficient-self-supervised-learning-with
NEXcepTion-S|2022|0.82|from-xception-to-nexception-new-design
NEXcepTion-TP|2022|0.818|from-xception-to-nexception-new-design
NEXcepTion-T|2022|0.815|from-xception-to-nexception-new-design
Adlik-ViT-SG+Swin_large+Convnext_xlarge|2022|0.8835999999999999|a-convnet-for-the-2020s
RevCol-H|2022|0.9|reversible-column-networks
mPLUG-2|2023|0.885|mplug-2-a-modularized-multi-modal-foundation
DeepMAD-89M|2023|0.84|deepmad-mathematical-architecture-design-for
BiFormer-B* |2023|0.8540000000000001|biformer-vision-transformer-with-bi-level
BiFormer-S* |2023|0.843|biformer-vision-transformer-with-bi-level
BiFormer-T |2023|0.8140000000000001|biformer-vision-transformer-with-bi-level
ViC-MAE |2023|0.85|visual-representation-learning-from-unlabeled
MAWS |2023|0.9009999999999999|the-effectiveness-of-mae-pre-pretraining-for
MAWS |2023|0.898|the-effectiveness-of-mae-pre-pretraining-for
MAWS |2023|0.895|the-effectiveness-of-mae-pre-pretraining-for
MAWS |2023|0.888|the-effectiveness-of-mae-pre-pretraining-for
MAWS |2023|0.868|the-effectiveness-of-mae-pre-pretraining-for
Diffusion Classifier|2023|0.7909999999999999|your-diffusion-model-is-secretly-a-zero-shot
CloFormer-S|2023|0.816|rethinking-local-perception-in-lightweight
CloFormer-XS|2023|0.7979999999999999|rethinking-local-perception-in-lightweight
CloFormer-XXS|2023|0.77|rethinking-local-perception-in-lightweight
Unicom |2023|0.883|unicom-universal-and-compact-representation
XCiT-M |2023|0.841|mixpro-data-augmentation-with-maskmix-and
CA-Swin-S |2023|0.8370000000000001|mixpro-data-augmentation-with-maskmix-and
DeiT-B |2023|0.8290000000000001|mixpro-data-augmentation-with-maskmix-and
CA-Swin-T |2023|0.828|mixpro-data-augmentation-with-maskmix-and
PVT-M |2023|0.8270000000000001|mixpro-data-augmentation-with-maskmix-and
PVT-S |2023|0.812|mixpro-data-augmentation-with-maskmix-and
CaiT-XXS |2023|0.8059999999999999|mixpro-data-augmentation-with-maskmix-and
PVT-T |2023|0.767|mixpro-data-augmentation-with-maskmix-and
DeiT-T |2023|0.738|mixpro-data-augmentation-with-maskmix-and
ONE-PEACE|2023|0.898|one-peace-exploring-one-general
Hiera-H|2023|0.8690000000000001|hiera-a-hierarchical-vision-transformer
Swin-T+SSA|2023|0.8189|the-information-pathways-hypothesis
ViT-H @224 |2023|0.857|augmenting-sub-model-to-improve-main-model
ViT-L @224 |2023|0.853|augmenting-sub-model-to-improve-main-model
ViT-B @224 |2023|0.8420000000000001|augmenting-sub-model-to-improve-main-model
CaiT-S24|2023|0.8491|which-transformer-to-favor-a-comparative
XCiT-S|2023|0.8365|which-transformer-to-favor-a-comparative
Wave-ViT-S|2023|0.8361|which-transformer-to-favor-a-comparative
SwinV2-Ti|2023|0.8309000000000001|which-transformer-to-favor-a-comparative
ViT-S|2023|0.8254|which-transformer-to-favor-a-comparative
EViT |2023|0.8229000000000001|which-transformer-to-favor-a-comparative
STViT-Swin-Ti|2023|0.8222|which-transformer-to-favor-a-comparative
ToMe-ViT-S|2023|0.8210999999999999|which-transformer-to-favor-a-comparative
EViT |2023|0.8195999999999999|which-transformer-to-favor-a-comparative
GFNet-S|2023|0.8133|which-transformer-to-favor-a-comparative
DynamicViT-S|2023|0.8109000000000001|which-transformer-to-favor-a-comparative
TokenLearner-ViT-8|2023|0.8066|which-transformer-to-favor-a-comparative
CoaT-Ti|2023|0.7842|which-transformer-to-favor-a-comparative
Poly-SA-ViT-S|2023|0.7834|which-transformer-to-favor-a-comparative
EfficientFormer-V2-S0|2023|0.7153|which-transformer-to-favor-a-comparative
DAT-B++ |2023|0.8590000000000001|dat-spatially-dynamic-vision-transformer-with
DAT-B++ |2023|0.8490000000000001|dat-spatially-dynamic-vision-transformer-with
DAT-S++|2023|0.846|dat-spatially-dynamic-vision-transformer-with
DAT-T++|2023|0.8390000000000001|dat-spatially-dynamic-vision-transformer-with
MIRL|2023|0.848|masked-image-residual-learning-for-scaling-1
MIRL |2023|0.862|masked-image-residual-learning-for-scaling-1
GTP-ViT-B-Patch8/P20|2023|0.858|gtp-vit-efficient-vision-transformers-via
GTP-EVA-L/P8|2023|0.8540000000000001|gtp-vit-efficient-vision-transformers-via
GTP-ViT-L/P8|2023|0.8370000000000001|gtp-vit-efficient-vision-transformers-via
GTP-LV-ViT-M/P8|2023|0.828|gtp-vit-efficient-vision-transformers-via
GTP-LV-ViT-S/P8|2023|0.8190000000000001|gtp-vit-efficient-vision-transformers-via
GTP-DeiT-B/P8|2023|0.815|gtp-vit-efficient-vision-transformers-via
GTP-DeiT-S/P8|2023|0.795|gtp-vit-efficient-vision-transformers-via
UniRepLKNet-XL++|2023|0.88|unireplknet-a-universal-perception-large
UniRepLKNet-L++|2023|0.879|unireplknet-a-universal-perception-large
UniRepLKNet-B++|2023|0.8740000000000001|unireplknet-a-universal-perception-large
UniRepLKNet-S++|2023|0.8640000000000001|unireplknet-a-universal-perception-large
UniRepLKNet-S|2023|0.8390000000000001|unireplknet-a-universal-perception-large
UniRepLKNet-T|2023|0.8320000000000001|unireplknet-a-universal-perception-large
UniRepLKNet-N|2023|0.816|unireplknet-a-universal-perception-large
UniRepLKNet-P|2023|0.802|unireplknet-a-universal-perception-large
UniRepLKNet-F|2023|0.7859999999999999|unireplknet-a-universal-perception-large
UniRepLKNet-A|2023|0.77|unireplknet-a-universal-perception-large
AIM-7B|2024|0.84|scalable-pre-training-of-large-autoregressive
#ddd|"""yAx"|false|.891
ru|"""tre"|"""line"""|#21ccc7
Inception ResNet V2|2016|0.8009999999999999|inception-v4-inception-resnet-and-the-impact
PNASNet-5|2017|0.8290000000000001|progressive-neural-architecture-search
ResNeXt-101 32x48d|2018|0.8540000000000001|exploring-the-limits-of-weakly-supervised
EfficientNet-B8 |2019|0.8540000000000001|randaugment-practical-data-augmentation-with
BiT-L |2019|0.8754000000000001|large-scale-learning-of-general-visual
ru|"""nam"|"""scatter"""|{name
DenseNet-169|2016|0.762|densely-connected-convolutional-networks
ResNeXt-101 32\u00d716d|2018|0.8420000000000001|exploring-the-limits-of-weakly-supervised
MnasNet-A3|2018|0.767|mnasnet-platform-aware-neural-architecture
MnasNet-A2|2018|0.7559999999999999|mnasnet-platform-aware-neural-architecture
Oct-ResNet-152 |2019|0.8290000000000001|drop-an-octave-reducing-spatial-redundancy-in
EfficientNet-B7|2019|0.8440000000000001|efficientnet-rethinking-model-scaling-for
EfficientNet-B5|2019|0.833|efficientnet-rethinking-model-scaling-for
EfficientNet-B7 |2019|0.85|randaugment-practical-data-augmentation-with
ResNet-50|2019|0.721|on-the-adequacy-of-untuned-warmup-for
InceptionV3 |2019|0.7895|filter-response-normalization-layer
ResnetV2 50 |2019|0.7720999999999999|filter-response-normalization-layer
BiT-M |2019|0.8539|large-scale-learning-of-general-visual
FixEfficientNet-B6|2020|0.867|fixing-the-train-test-resolution-discrepancy-2
FixEfficientNet-B4|2020|0.8590000000000001|fixing-the-train-test-resolution-discrepancy-2
FixEfficientNet-B8|2020|0.857|fixing-the-train-test-resolution-discrepancy-2
ResNeSt-200|2020|0.8390000000000001|resnest-split-attention-networks
ResNeSt-50-fast|2020|0.8064|resnest-split-attention-networks
ResNet200_vd_26w_4s_ssld|2020|0.851|semi-supervised-recognition-under-a-noisy-and
ReXNet-R_3.0|2020|0.845|rexnet-diminishing-representational
ReXNet-R_2.0|2020|0.8320000000000001|rexnet-diminishing-representational
ReXNet_2.0|2020|0.816|rexnet-diminishing-representational
ResNet-152 |2020|0.816|sharpness-aware-minimization-for-efficiently-1
DeiT-B 384|2020|0.852|training-data-efficient-image-transformers
DeiT-B|2020|0.8420000000000001|training-data-efficient-image-transformers
DeiT-B|2020|0.826|training-data-efficient-image-transformers
SENet-101|2021|0.8140000000000001|bottleneck-transformers-for-visual
T2T-ViTt-24|2021|0.826|tokens-to-token-vit-training-vision
NFNet-F5 w/ SAM|2021|0.863|high-performance-large-scale-image
NFNet-F5|2021|0.86|high-performance-large-scale-image
NFNet-F4|2021|0.8590000000000001|high-performance-large-scale-image
NFNet-F2|2021|0.851|high-performance-large-scale-image
ResNet-RS-50 |2021|0.8440000000000001|revisiting-resnets-improved-training-and
ResNet-34 |2021|0.7609999999999999|automix-unveiling-the-power-of-mixup
EfficientNetV2-L |2021|0.868|efficientnetv2-smaller-models-and-faster
EfficientNetV2-M |2021|0.861|efficientnetv2-smaller-models-and-faster
EfficientNetV2-L|2021|0.857|efficientnetv2-smaller-models-and-faster
EfficientNetV2-M|2021|0.851|efficientnetv2-smaller-models-and-faster
EfficientNetV2-S |2021|0.85|efficientnetv2-smaller-models-and-faster
EfficientNetV2-S|2021|0.8390000000000001|efficientnetv2-smaller-models-and-faster
LeViT-384|2021|0.825|levit-a-vision-transformer-in-convnet-s
ResNet-152x2-SAM|2021|0.8109999999999999|when-vision-transformers-outperform-resnets
CSWin-L |2021|0.875|cswin-transformer-a-general-vision
ResNet-152|2021|0.813|parametric-contrastive-learning
DeiT-B with iRPE-K|2021|0.8240000000000001|rethinking-and-improving-relative-position
ResNet-152 |2021|0.8240000000000001|resnet-strikes-back-an-improved-training
ResNet-152 |2021|0.818|resnet-strikes-back-an-improved-training
ResNet-101 |2021|0.8108|boosting-discriminative-visual-representation
ResNet-34 |2021|0.7635|boosting-discriminative-visual-representation
Dspike |2021|0.7123999999999999|differentiable-spike-rethinking-gradient
MViTv2-L |2021|0.863|improved-multiscale-vision-transformers-for
ELSA-VOLO-D5 |2021|0.872|elsa-enhanced-local-self-attention-for-vision
PatchConvNet-B60-21k-384|2021|0.865|augmenting-convolutional-networks-with
VOLO-D5+HAT|2022|0.873|improving-vision-transformers-by-revisiting
Mini-Swin-B@384|2022|0.855|minivit-compressing-vision-transformers-with
EfficientNetV2 |2022|0.872|polyloss-a-polynomial-expansion-perspective-1
TransBoost-MobileNetV3-L|2022|0.7681|transboost-improving-the-best-imagenet
RevBiFPN-S6|2022|0.8420000000000001|revbifpn-the-fully-reversible-bidirectional
Next-ViT-B|2022|0.8320000000000001|next-vit-next-generation-vision-transformer
Next-ViT-S|2022|0.825|next-vit-next-generation-vision-transformer
DiNAT_s-Large |2022|0.8740000000000001|dilated-neighborhood-attention-transformer
AIM-7B|2024|0.84|scalable-pre-training-of-large-autoregressive
#ddd|"""yAx"|false|.957
ru|"""tre"|"""line"""|#21ccc7
ResNet-152|2015|0.7857|deep-residual-learning-for-image-recognition
ResNet-200|2016|0.799|identity-mappings-in-deep-residual-networks
SimpleNetV1-9m-correct-labels|2016|0.8123999999999999|lets-keep-it-simple-using-simple
NASNET-A|2017|0.8270000000000001|learning-transferable-architectures-for
AmoebaNet-A|2018|0.8390000000000001|regularized-evolution-for-image-classifier
ResNeXt-101 32x32d|2018|0.851|exploring-the-limits-of-weakly-supervised
FixResNeXt-101 32x48d|2019|0.8640000000000001|fixing-the-train-test-resolution-discrepancy
NoisyStudent |2019|0.8690000000000001|self-training-with-noisy-student-improves
NoisyStudent |2020|0.884|self-training-with-noisy-student-improves
FixEfficientNet-L2|2020|0.885|fixing-the-train-test-resolution-discrepancy-2
Meta Pseudo Labels |2020|0.902|meta-pseudo-labels
ViT-G/14|2021|0.9045000000000001|scaling-vision-transformers
CoAtNet-7|2021|0.9087999999999999|coatnet-marrying-convolution-and-attention
Model soups |2022|0.9098|model-soups-averaging-weights-of-multiple
CoCa |2022|0.91|coca-contrastive-captioners-are-image-text
OmniVec|2023|0.924|omnivec-learning-robust-representations-with
ru|"""nam"|"""scatter"""|{name
ResNet-101|2015|0.7825|deep-residual-learning-for-image-recognition
WRN-50-2-bottleneck|2016|0.7809999999999999|wide-residual-networks
FractalNet-34|2016|0.7587999999999999|fractalnet-ultra-deep-neural-networks-without
SimpleNetV1-5m-correct-labels|2016|0.7912|lets-keep-it-simple-using-simple
SimpleNetV1-small-075-correct-labels|2016|0.7565999999999999|lets-keep-it-simple-using-simple
SimpleNetV1-9m|2016|0.7417|lets-keep-it-simple-using-simple
SimpleNetV1-5m|2016|0.7193999999999999|lets-keep-it-simple-using-simple
SimpleNetV1-small-05-correct-labels|2016|0.6911|lets-keep-it-simple-using-simple
SimpleNetV1-small-075|2016|0.6815000000000001|lets-keep-it-simple-using-simple
SimpleNetV1-small-05|2016|0.6152000000000001|lets-keep-it-simple-using-simple
DenseNet-201|2016|0.7742|densely-connected-convolutional-networks
DenseNet-121|2016|0.7498|densely-connected-convolutional-networks
Xception|2016|0.79|xception-deep-learning-with-depthwise
ResNeXt-101  64x4|2016|0.809|aggregated-residual-transformations-for-deep
MobileNet-224 \u00d71.25|2017|0.706|mobilenets-efficient-convolutional-neural
Attention-92|2017|0.805|residual-attention-network-for-image
ShuffleNet|2017|0.7090000000000001|shufflenet-an-extremely-efficient
ResNet-101 |2017|0.792|revisiting-unreasonable-effectiveness-of-data
MobileNetV2 |2018|0.747|mobilenetv2-inverted-residuals-and-linear
MobileNetV2|2018|0.72|mobilenetv2-inverted-residuals-and-linear
ResNet-152 + SWA|2018|0.7894|averaging-weights-leads-to-wider-optima-and
DenseNet-161 + SWA|2018|0.7844|averaging-weights-leads-to-wider-optima-and
Inception v3|2018|0.7712|what-do-deep-networks-like-to-see
ResNeXt-101 32x8d|2018|0.8220000000000001|exploring-the-limits-of-weakly-supervised
CoordConv ResNet-50|2018|0.7574|an-intriguing-failing-of-convolutional-neural
ShuffleNet V2|2018|0.754|shufflenet-v2-practical-guidelines-for
MnasNet-A1|2018|0.752|mnasnet-platform-aware-neural-architecture
GPIPE|2018|0.8440000000000001|gpipe-efficient-training-of-giant-neural
ESPNetv2|2018|0.7490000000000001|espnetv2-a-light-weight-power-efficient-and
Proxyless|2018|0.746|proxylessnas-direct-neural-architecture
ResNet-50-D|2018|0.7716|bag-of-tricks-for-image-classification-with
FBNet-C|2018|0.7490000000000001|fbnet-hardware-aware-efficient-convnet-design
ColorNet |2019|0.8432|colornet-investigating-the-importance-of
ColorNet|2019|0.8234999999999999|colornet-investigating-the-importance-of
MultiGrain PNASNet |2019|0.836|multigrain-a-unified-image-embedding-for
MultiGrain PNASNet |2019|0.8320000000000001|multigrain-a-unified-image-embedding-for
MultiGrain SENet154 |2019|0.831|multigrain-a-unified-image-embedding-for
MultiGrain SENet154 |2019|0.83|multigrain-a-unified-image-embedding-for
MultiGrain SENet154 |2019|0.8270000000000001|multigrain-a-unified-image-embedding-for
MultiGrain PNASNet |2019|0.826|multigrain-a-unified-image-embedding-for
MultiGrain PNASNet |2019|0.813|multigrain-a-unified-image-embedding-for
MultiGrain R50-AA-500|2019|0.794|multigrain-a-unified-image-embedding-for
MultiGrain R50-AA-224|2019|0.782|multigrain-a-unified-image-embedding-for
MultiGrain NASNet-A-Mobile |2019|0.7509999999999999|multigrain-a-unified-image-embedding-for
Graph-RISE |2019|0.6829000000000001|graph-rise-graph-regularized-image-semantic
SKNet-101|2019|0.7981|selective-kernel-networks
SRM-ResNet-101|2019|0.7847|srm-a-style-based-recalibration-module-for
Res2Net-101|2019|0.8123|res2net-a-new-multi-scale-backbone
RandWire-WS|2019|0.8009999999999999|exploring-randomly-wired-neural-networks-for
Res2Net-50-299|2019|0.7859|res2net-a-new-multi-scale-backbone
RandWire-WS |2019|0.747|exploring-randomly-wired-neural-networks-for
Single-Path NAS|2019|0.7495999999999999|single-path-nas-designing-hardware-efficient
ACNet |2019|0.775|adaptively-connected-neural-networks
EfficientNet-B0 |2019|0.7829999999999999|soft-conditional-computation
ScaleNet-152|2019|0.7938|190409460
ScaleNet-101|2019|0.7903|190409460
ScaleNet-50|2019|0.778|190409460
AA-ResNet-152|2019|0.7909999999999999|190409925
LR-Net-26|2019|0.757|190411491
ResNet-50 |2019|0.7904000000000001|unsupervised-data-augmentation-1
ResNet-200 |2019|0.8059999999999999|fast-autoaugment
ResNet-50 |2019|0.7759999999999999|fast-autoaugment
ResNeXt-101 32x16d |2019|0.848|billion-scale-semi-supervised-learning-for
ResNeXt-101 32x8d |2019|0.843|billion-scale-semi-supervised-learning-for
ResNeXt-101 32x4d |2019|0.8340000000000001|billion-scale-semi-supervised-learning-for
MobileNet V3-Large 1.0|2019|0.752|searching-for-mobilenetv3
ResNeXt-101 |2019|0.8053|cutmix-regularization-strategy-to-train
ResNet-50 |2019|0.784|cutmix-regularization-strategy-to-train
SGE-ResNet101|2019|0.78798|spatial-group-wise-enhance-improving-semantic
SGE-ResNet50|2019|0.7758400000000001|spatial-group-wise-enhance-improving-semantic
EfficientNet-B6|2019|0.84|efficientnet-rethinking-model-scaling-for
EfficientNet-B4|2019|0.826|efficientnet-rethinking-model-scaling-for
EfficientNet-B3|2019|0.8109999999999999|efficientnet-rethinking-model-scaling-for
EfficientNet-B2|2019|0.7979999999999999|efficientnet-rethinking-model-scaling-for
EfficientNet-B1|2019|0.7879999999999999|efficientnet-rethinking-model-scaling-for
EfficientNet-B0|2019|0.763|efficientnet-rethinking-model-scaling-for
DiCENet|2019|0.7509999999999999|dicenet-dimension-wise-convolutions-for
FixResNet-50 Billion-scale@224|2019|0.825|fixing-the-train-test-resolution-discrepancy
FixResNet-50 CutMix|2019|0.7979999999999999|fixing-the-train-test-resolution-discrepancy
FixResNet-50|2019|0.7909999999999999|fixing-the-train-test-resolution-discrepancy
DenseNAS-A|2019|0.759|densely-connected-search-space-for-more
FairNAS-A|2019|0.7534000000000001|fairnas-rethinking-evaluation-fairness-of
FairNAS-B|2019|0.7509999999999999|fairnas-rethinking-evaluation-fairness-of
FairNAS-C|2019|0.7469|fairnas-rethinking-evaluation-fairness-of
MixNet-L|2019|0.789|mixnet-mixed-depthwise-convolutional-kernels
MixNet-M|2019|0.77|mixnet-mixed-depthwise-convolutional-kernels
MixNet-S|2019|0.758|mixnet-mixed-depthwise-convolutional-kernels
MobileNet-224 |2019|0.7256|compact-global-descriptor-for-neural-networks
AOGNet-40M-AN|2019|0.8187000000000001|attentive-normalization
MoGA-A|2019|0.759|moga-searching-beyond-mobilenetv3
LIP-ResNet-101|2019|0.7933|lip-local-importance-based-pooling
ResNet-50 |2019|0.7815000000000001|lip-local-importance-based-pooling
LIP-DenseNet-BC-121|2019|0.7664|lip-local-importance-based-pooling
SCARLET-A4|2019|0.823|scarletnas-bridging-the-gap-between
SCARLET-A|2019|0.769|scarletnas-bridging-the-gap-between
SCARLET-B|2019|0.763|scarletnas-bridging-the-gap-between
SCARLET-C|2019|0.7559999999999999|scarletnas-bridging-the-gap-between
CSPResNeXt-50 + Mish|2019|0.7979999999999999|mish-a-self-regularized-non-monotonic-neural
HCGNet-C|2019|0.805|gated-convolutional-networks-with-hybrid
HCGNet-B|2019|0.785|gated-convolutional-networks-with-hybrid
BBG |2019|0.626|balanced-binary-neural-networks-with-gated
BBG |2019|0.594|balanced-binary-neural-networks-with-gated
ResNet-50-DW |2019|0.785|deformable-kernels-adapting-effective
ECA-Net |2019|0.7892|eca-net-efficient-channel-attention-for-deep
ECA-Net |2019|0.7865000000000001|eca-net-efficient-channel-attention-for-deep
ECA-Net |2019|0.7748|eca-net-efficient-channel-attention-for-deep
ECA-Net |2019|0.7256|eca-net-efficient-channel-attention-for-deep
NoisyStudent |2019|0.8640000000000001|self-training-with-noisy-student-improves
NoisyStudent |2019|0.861|self-training-with-noisy-student-improves
NoisyStudent |2019|0.853|self-training-with-noisy-student-improves
NoisyStudent |2019|0.841|self-training-with-noisy-student-improves
NoisyStudent |2019|0.8240000000000001|self-training-with-noisy-student-improves
NoisyStudent |2019|0.815|self-training-with-noisy-student-improves
NoisyStudent |2019|0.7879999999999999|self-training-with-noisy-student-improves
AdvProp |2019|0.855|adversarial-examples-improve-image
AdvProp |2019|0.852|adversarial-examples-improve-image
CSPResNeXt-50 |2019|0.7979999999999999|cspnet-a-new-backbone-that-can-enhance
GhostNet \u00d71.3|2019|0.757|ghostnet-more-features-from-cheap-operations
Ghost-ResNet-50 |2019|0.75|ghostnet-more-features-from-cheap-operations
Ghost-ResNet-50 |2019|0.741|ghostnet-more-features-from-cheap-operations
GhostNet \u00d71.0|2019|0.7390000000000001|ghostnet-more-features-from-cheap-operations
GhostNet \u00d70.5|2019|0.662|ghostnet-more-features-from-cheap-operations
Wide ResNet-50 |2019|0.733|whats-hidden-in-a-randomly-weighted-neural
DY-MobileNetV2 \u00d71.0|2019|0.7440000000000001|dynamic-convolution-attention-over
DY-MobileNetV2 \u00d70.75|2019|0.728|dynamic-convolution-attention-over
DY-ResNet-18|2019|0.727|dynamic-convolution-attention-over
DY-MobileNetV3-Small|2019|0.6970000000000001|dynamic-convolution-attention-over
DY-MobileNetV2 \u00d70.5|2019|0.6940000000000001|dynamic-convolution-attention-over
DY-ResNet-10|2019|0.677|dynamic-convolution-attention-over
DY-MobileNetV2 \u00d70.35|2019|0.649|dynamic-convolution-attention-over
SpineNet-143|2019|0.79|spinenet-learning-scale-permuted-backbone-for
ResNet-200 |2019|0.8131999999999999|adversarial-autoaugment-1
ResNet-50 |2019|0.794|adversarial-autoaugment-1
Assemble-ResNet152|2020|0.8420000000000001|compounding-the-performance-improvements-of
Fix-EfficientNet-B8 |2020|0.858|maxup-a-simple-way-to-improve-generalization
FixEfficientNet-B7|2020|0.871|fixing-the-train-test-resolution-discrepancy-2
FixEfficientNet-B5|2020|0.8640000000000001|fixing-the-train-test-resolution-discrepancy-2
FixEfficientNet-B3|2020|0.85|fixing-the-train-test-resolution-discrepancy-2
FixEfficientNetB4|2020|0.84|fixing-the-train-test-resolution-discrepancy-2
FixEfficientNet-B2|2020|0.836|fixing-the-train-test-resolution-discrepancy-2
FixEfficientNet-B1|2020|0.826|fixing-the-train-test-resolution-discrepancy-2
FixEfficientNet-B0|2020|0.802|fixing-the-train-test-resolution-discrepancy-2
Meta Pseudo Labels |2020|0.9|meta-pseudo-labels
Meta Pseudo Labels |2020|0.8320000000000001|meta-pseudo-labels
KDforAA |2020|0.858|circumventing-outliers-of-autoaugment-with
KDforAA |2020|0.855|circumventing-outliers-of-autoaugment-with
GreedyNAS-A|2020|0.7709999999999999|greedynas-towards-fast-one-shot-nas-with
GreedyNAS-B|2020|0.768|greedynas-towards-fast-one-shot-nas-with
GreedyNAS-C|2020|0.762|greedynas-towards-fast-one-shot-nas-with
TResNet-XL|2020|0.843|tresnet-high-performance-gpu-dedicated
RegNetY-8.0GF|2020|0.799|designing-network-design-spaces
RegNetY-4.0GF|2020|0.794|designing-network-design-spaces
RegNetY-1.6GF|2020|0.78|designing-network-design-spaces
RegNetY-800MF|2020|0.763|designing-network-design-spaces
RegNetY-600MF|2020|0.755|designing-network-design-spaces
RegNetY-400MF|2020|0.741|designing-network-design-spaces
MUXNet-l|2020|0.7659999999999999|muxconv-information-multiplexing-in
MUXNet-m|2020|0.753|muxconv-information-multiplexing-in
MUXNet-s|2020|0.716|muxconv-information-multiplexing-in
MUXNet-xs|2020|0.667|muxconv-information-multiplexing-in
ResNeSt-269|2020|0.845|resnest-split-attention-networks
ResNeSt-101|2020|0.83|resnest-split-attention-networks
ResNeSt-50|2020|0.8112999999999999|resnest-split-attention-networks
ResNet-200 |2020|0.8079999999999999|supervised-contrastive-learning
NAT-M4|2020|0.805|neural-architecture-transfer
Multiscale DEQ |2020|0.792|multiscale-deep-equilibrium-models
Fix_ResNet50_vd_ssld|2020|0.84|semi-supervised-recognition-under-a-noisy-and
ResNet50_vd_ssld|2020|0.83|semi-supervised-recognition-under-a-noisy-and
MobileNetV3_large_x1_0_ssld|2020|0.79|semi-supervised-recognition-under-a-noisy-and
PyConvResNet-101|2020|0.8149|pyramidal-convolution-rethinking
Prodpoly|2020|0.7717|deep-polynomial-neural-networks
PS-KD |2020|0.7924|self-knowledge-distillation-a-simple-way-for
ReXNet_3.0|2020|0.828|rexnet-diminishing-representational
ReXNet_1.5|2020|0.8029999999999999|rexnet-diminishing-representational
ReXNet_1.3|2020|0.795|rexnet-diminishing-representational
ReXNet_1.0|2020|0.779|rexnet-diminishing-representational
ReXNet_0.9|2020|0.772|rexnet-diminishing-representational
ReXNet_0.6|2020|0.746|rexnet-diminishing-representational
Ours|2020|0.7197|quantnet-learning-to-quantize-by-learning
ResNet-50|2020|0.7876000000000001|puzzle-mix-exploiting-saliency-and-local-1
MEAL V2 |2020|0.8172|meal-v2-boosting-vanilla-resnet-50-to-80-top
MEAL V2 |2020|0.8067|meal-v2-boosting-vanilla-resnet-50-to-80-top
ResNet-18 |2020|0.7319|meal-v2-boosting-vanilla-resnet-50-to-80-top
iAFF-ResNeXt-50-32x4d|2020|0.8022|attentional-feature-fusion
EfficientNet-L2-475 |2020|0.8861|sharpness-aware-minimization-for-efficiently-1
ResNeXt-101 |2020|0.812|shape-texture-debiased-neural-network-1
ViT-H/14|2020|0.8855|an-image-is-worth-16x16-words-transformers-1
ViT-L/16|2020|0.8776|an-image-is-worth-16x16-words-transformers-1
TinyNet |2020|0.794|model-rubik-s-cube-twisting-resolution-depth
TinyNet-A + RA|2020|0.777|model-rubik-s-cube-twisting-resolution-depth
Grafit |2020|0.7959999999999999|grafit-learning-fine-grained-image
ResNet-18 |2020|0.7171|torchdistill-a-modular-configuration-driven
ResNet-18 |2020|0.7156|torchdistill-a-modular-configuration-driven
ResNet-18 |2020|0.7137|torchdistill-a-modular-configuration-driven
ResNet-18 |2020|0.7108|torchdistill-a-modular-configuration-driven
ResNet-18 |2020|0.7093|torchdistill-a-modular-configuration-driven
ResNet-18 |2020|0.7051999999999999|torchdistill-a-modular-configuration-driven
ResNet-18 |2020|0.7009000000000001|torchdistill-a-modular-configuration-driven
SE-ResNeXt-10|2020|0.836|SE-ResNeXt-10
SE-ResNeXt-10|2020|0.8334|SE-ResNeXt-10
ResNeXt-10|2020|0.8212999999999999|ResNeXt-10
DeiT-B|2020|0.7659999999999999|training-data-efficient-image-transformers
ResNet-50+AutoDropout+RandAugment|2021|0.8029999999999999|autodropout-learning-dropout-patterns-to
ResNet-50|2021|0.787|autodropout-learning-dropout-patterns-to
EfficientNet-B0|2021|0.775|autodropout-learning-dropout-patterns-to
SSAL-Resnet50|2021|0.77|contextual-classification-using-self
RepVGG-B2|2021|0.7878000000000001|repvgg-making-vgg-style-convnets-great-again
RepVGG-B2g4|2021|0.785|repvgg-making-vgg-style-convnets-great-again
ReXNet_1.0-relabel|2021|0.784|re-labeling-imagenet-from-single-to-multi
BoTNet T7|2021|0.847|bottleneck-transformers-for-visual
BoTNet T7-320|2021|0.8420000000000001|bottleneck-transformers-for-visual
BoTNet T6|2021|0.84|bottleneck-transformers-for-visual
SENet-350|2021|0.838|bottleneck-transformers-for-visual
BoTNet T5|2021|0.835|bottleneck-transformers-for-visual
BoTNet T4|2021|0.828|bottleneck-transformers-for-visual
SENet-152|2021|0.8220000000000001|bottleneck-transformers-for-visual
BoTNet T3|2021|0.8170000000000001|bottleneck-transformers-for-visual
ResNet-101|2021|0.8|bottleneck-transformers-for-visual
SENet-50|2021|0.794|bottleneck-transformers-for-visual
ResNet-50|2021|0.7879999999999999|bottleneck-transformers-for-visual
"T2T-ViT-14|384"|2021|0.833|tokens-to-token-vit-training-vision
T2T-ViT-24|2021|0.823|tokens-to-token-vit-training-vision
T2T-ViTt-19|2021|0.8220000000000001|tokens-to-token-vit-training-vision
T2T-ViT-19|2021|0.8190000000000001|tokens-to-token-vit-training-vision
T2T-ViT-14|2021|0.815|tokens-to-token-vit-training-vision
ZenNAS |2021|0.83|zen-nas-a-zero-shot-nas-for-high-performance
ZenNet-400M-SE|2021|0.78|zen-nas-a-zero-shot-nas-for-high-performance
NFNet-F4+|2021|0.892|high-performance-large-scale-image
ALIGN |2021|0.8864|scaling-up-visual-and-vision-language
NFNet-F6 w/ SAM|2021|0.865|high-performance-large-scale-image
NFNet-F3|2021|0.857|high-performance-large-scale-image
NFNet-F1|2021|0.847|high-performance-large-scale-image
NFNet-F0|2021|0.836|high-performance-large-scale-image
ResNet-50 MLPerf v0.7 - 2512 steps|2021|0.7592|a-large-batch-optimizer-reality-check
AlphaNet-A6|2021|0.8079999999999999|alphanet-improved-training-of-supernet-with
AlphaNet-A5|2021|0.8029999999999999|alphanet-improved-training-of-supernet-with
AlphaNet-A4|2021|0.8|alphanet-improved-training-of-supernet-with
AlphaNet-A3|2021|0.794|alphanet-improved-training-of-supernet-with
AlphaNet-A2|2021|0.7909999999999999|alphanet-improved-training-of-supernet-with
AlphaNet-A1|2021|0.789|alphanet-improved-training-of-supernet-with
AlphaNet-A0|2021|0.778|alphanet-improved-training-of-supernet-with
LambdaResNet200|2021|0.843|lambdanetworks-modeling-long-range-1
LambdaResNet152|2021|0.84|lambdanetworks-modeling-long-range-1
TNT-B|2021|0.8390000000000001|transformer-in-transformer
Perceiver |2021|0.78|perceiver-general-perception-with-iterative
Perceiver|2021|0.764|perceiver-general-perception-with-iterative
RedNet-152|2021|0.7929999999999999|involution-inverting-the-inherence-of
RedNet-101|2021|0.7909999999999999|involution-inverting-the-inherence-of
RedNet-50|2021|0.784|involution-inverting-the-inherence-of
RedNet-38|2021|0.7759999999999999|involution-inverting-the-inherence-of
RedNet-26|2021|0.759|involution-inverting-the-inherence-of
ResNet-RS-270 |2021|0.838|revisiting-resnets-improved-training-and
ConViT-B+|2021|0.825|convit-improving-vision-transformers-with
ConViT-B|2021|0.8240000000000001|convit-improving-vision-transformers-with
ConViT-S+|2021|0.8220000000000001|convit-improving-vision-transformers-with
ConViT-S|2021|0.813|convit-improving-vision-transformers-with
HVT-S-1|2021|0.78|scalable-visual-transformers-with
ConViT-Ti+|2021|0.767|convit-improving-vision-transformers-with
ConViT-Ti|2021|0.731|convit-improving-vision-transformers-with
HVT-Ti-1|2021|0.6964|scalable-visual-transformers-with
CeiT-S |2021|0.833|incorporating-convolution-designs-into-visual
DeepVit-L* |2021|0.831|deepvit-towards-deeper-vision-transformer
DeepVit-L|2021|0.8220000000000001|deepvit-towards-deeper-vision-transformer
CeiT-S|2021|0.82|incorporating-convolution-designs-into-visual
CeiT-T |2021|0.7879999999999999|incorporating-convolution-designs-into-visual
CeiT-T|2021|0.764|incorporating-convolution-designs-into-visual
BossNet-T1|2021|0.8220000000000001|bossnas-exploring-hybrid-cnn-transformers
ResNet-101 |2021|0.8098000000000001|automix-unveiling-the-power-of-mixup
ResNet-50 |2021|0.7925|automix-unveiling-the-power-of-mixup
ResNet-18 |2021|0.7204999999999999|automix-unveiling-the-power-of-mixup
Swin-T|2021|0.813|swin-transformer-hierarchical-vision
CrossViT-18+|2021|0.828|2103-14899
CrossViT-18|2021|0.825|2103-14899
CrossViT-15+|2021|0.823|2103-14899
CrossViT-15|2021|0.815|2103-14899
CvT-21 |2021|0.833|cvt-introducing-convolutions-to-vision
ViL-Medium-D|2021|0.833|2103-15358
ViL-Base-D|2021|0.8320000000000001|2103-15358
CvT-13 |2021|0.83|cvt-introducing-convolutions-to-vision
ViL-Medium-W|2021|0.8290000000000001|2103-15358
CvT-21|2021|0.825|cvt-introducing-convolutions-to-vision
CvT-13-NAS|2021|0.8220000000000001|cvt-introducing-convolutions-to-vision
ViL-Small|2021|0.82|2103-15358
ViL-Base-W|2021|0.8190000000000001|2103-15358
CvT-13|2021|0.816|cvt-introducing-convolutions-to-vision
ViL-Tiny-RPB|2021|0.767|2103-15358
PiT-B|2021|0.84|rethinking-spatial-dimensions-of-vision
PiT-S|2021|0.8190000000000001|rethinking-spatial-dimensions-of-vision
PiT-XS|2021|0.7909999999999999|rethinking-spatial-dimensions-of-vision
PiT-Ti|2021|0.746|rethinking-spatial-dimensions-of-vision
CaiT-M-48-448|2021|0.865|going-deeper-with-image-transformers
CAIT-M36-448|2021|0.863|going-deeper-with-image-transformers
CAIT-M-36|2021|0.861|going-deeper-with-image-transformers
CAIT-M-24|2021|0.858|going-deeper-with-image-transformers
CAIT-S-36|2021|0.8540000000000001|going-deeper-with-image-transformers
CAIT-S-48|2021|0.853|going-deeper-with-image-transformers
CAIT-S-24|2021|0.851|going-deeper-with-image-transformers
CAIT-XS-36|2021|0.848|going-deeper-with-image-transformers
CAIT-XS-24|2021|0.841|going-deeper-with-image-transformers
CAIT-XXS-36|2021|0.8220000000000001|going-deeper-with-image-transformers
CAIT-XXS-24|2021|0.809|going-deeper-with-image-transformers
LeViT-256|2021|0.816|levit-a-vision-transformer-in-convnet-s
LeViT-192|2021|0.8|levit-a-vision-transformer-in-convnet-s
LeViT-128|2021|0.7959999999999999|levit-a-vision-transformer-in-convnet-s
LeViT-128S|2021|0.757|levit-a-vision-transformer-in-convnet-s
"CCT-14/7x2 | 384"|2021|0.8271|escaping-the-big-data-paradigm-with-compact
CCT-14/7x2|2021|0.8134|escaping-the-big-data-paradigm-with-compact
LocalViT-S|2021|0.8079999999999999|localvit-bringing-locality-to-vision
CCT-16/7x2|2021|0.8028|escaping-the-big-data-paradigm-with-compact
LocalViT-PVT|2021|0.782|localvit-bringing-locality-to-vision
LocalViT-TNT|2021|0.759|localvit-bringing-locality-to-vision
LocalViT-T|2021|0.748|localvit-bringing-locality-to-vision
LocalViT-T2T|2021|0.725|localvit-bringing-locality-to-vision
AsymmNet-Large \u00d71.0|2021|0.754|asymmnet-towards-ultralight-convolution
AsymmNet-Large \u00d70.5|2021|0.6920000000000001|asymmnet-towards-ultralight-convolution
AsymmNet-Small \u00d71.0|2021|0.684|asymmnet-towards-ultralight-convolution
PDC|2021|0.716|polynomial-networks-in-deep-classifiers
ReActNet-A |2021|0.68|bnn-bn-training-binary-neural-networks
DIFFQ |2021|0.82|differentiable-model-compression-via-pseudo
LV-ViT-L|2021|0.8640000000000001|token-labeling-training-a-85-5-top-1-accuracy
MViT-B-24|2021|0.848|multiscale-vision-transformers
LV-ViT-M|2021|0.841|token-labeling-training-a-85-5-top-1-accuracy
LV-ViT-S|2021|0.833|token-labeling-training-a-85-5-top-1-accuracy
MViT-B-16|2021|0.83|multiscale-vision-transformers
Visformer-S|2021|0.8220000000000001|visformer-the-vision-friendly-transformer
Visformer-Ti|2021|0.7859999999999999|visformer-the-vision-friendly-transformer
Twins-SVT-L|2021|0.8370000000000001|twins-revisiting-spatial-attention-design-in
Mixer-H/14 |2021|0.8794|mlp-mixer-an-all-mlp-architecture-for-vision
ViT-L/16 Dosovitskiy et al. |2021|0.853|mlp-mixer-an-all-mlp-architecture-for-vision
Mixer-B/16|2021|0.7644|mlp-mixer-an-all-mlp-architecture-for-vision
T2T-ViT-14|2021|0.8170000000000001|beyond-self-attention-external-attention
RepMLP-Res50|2021|0.7859999999999999|repmlp-re-parameterizing-convolutions-into
FF|2021|0.7490000000000001|do-you-even-need-attention-a-stack-of-feed
ResMLP-B24/8|2021|0.836|resmlp-feedforward-networks-for-image
ResMLP-S24|2021|0.8079999999999999|resmlp-feedforward-networks-for-image
BasisNet-MV3|2021|0.8|basisnet-two-stage-model-synthesis-for-1
ResMLP-36|2021|0.797|resmlp-feedforward-networks-for-image
ResMLP-24|2021|0.794|resmlp-feedforward-networks-for-image
ResMLP-S12|2021|0.778|resmlp-feedforward-networks-for-image
Conformer-B|2021|0.841|conformer-local-features-coupling-global
RVT-B*|2021|0.8270000000000001|rethinking-the-design-principles-of-robust
RVT-S*|2021|0.8190000000000001|rethinking-the-design-principles-of-robust
gMLP-B|2021|0.816|pay-attention-to-mlps
RVT-Ti*|2021|0.792|rethinking-the-design-principles-of-robust
Heteroscedastic |2021|0.6859999999999999|correlated-input-dependent-label-noise-in
Transformer local-attention |2021|0.838|aggregating-nested-transformers
Transformer local-attention |2021|0.833|aggregating-nested-transformers
Transformer local-attention |2021|0.815|aggregating-nested-transformers
NFNet-F5 w/ SAM w/ augmult=16|2021|0.8678|drawing-multiple-augmentation-samples-per
ResT-Large|2021|0.836|rest-an-efficient-transformer-for-visual
ResT-Small|2021|0.7959999999999999|rest-an-efficient-transformer-for-visual
DVT |2021|0.8043|not-all-images-are-worth-16x16-words-dynamic
DVT |2021|0.7974|not-all-images-are-worth-16x16-words-dynamic
DVT |2021|0.7848|not-all-images-are-worth-16x16-words-dynamic
Container Container|2021|0.8270000000000001|container-context-aggregation-network
Container-Light|2021|0.82|container-context-aggregation-network
DynamicViT-LV-M/0.8|2021|0.8390000000000001|dynamicvit-efficient-vision-transformers-with
ViT-B/16-SAM|2021|0.799|when-vision-transformers-outperform-resnets
Mixer-B/8-SAM|2021|0.79|when-vision-transformers-outperform-resnets
Refiner-ViT-L|2021|0.8603000000000001|refiner-refining-self-attention-for-vision
ViTAE-B-Stage|2021|0.836|vitae-vision-transformer-advanced-by
ViTAE-S-Stage|2021|0.8220000000000001|vitae-vision-transformer-advanced-by
ViTAE-13M|2021|0.81|vitae-vision-transformer-advanced-by
ViTAE-6M|2021|0.779|vitae-vision-transformer-advanced-by
ViTAE-T-Stage|2021|0.768|vitae-vision-transformer-advanced-by
ViTAE-T|2021|0.753|vitae-vision-transformer-advanced-by
CoAtNet-6|2021|0.9045000000000001|coatnet-marrying-convolution-and-attention
CoAtNet-3 @384|2021|0.8852|coatnet-marrying-convolution-and-attention
FunMatch - T384+224 |2021|0.828|knowledge-distillation-a-good-teacher-is
V-MoE-H/14 |2021|0.8835999999999999|scaling-vision-with-sparse-mixture-of-experts
V-MoE-H/14 |2021|0.8823000000000001|scaling-vision-with-sparse-mixture-of-experts
VIT-H/14|2021|0.8808|scaling-vision-with-sparse-mixture-of-experts
V-MoE-L/16 |2021|0.8741|scaling-vision-with-sparse-mixture-of-experts
BEiT-L |2021|0.8859999999999999|beit-bert-pre-training-of-image-transformers
BEiT-L |2021|0.863|beit-bert-pre-training-of-image-transformers
XCiT-L24|2021|0.86|xcit-cross-covariance-image-transformers
XCiT-M24|2021|0.858|xcit-cross-covariance-image-transformers
XCiT-S24|2021|0.856|xcit-cross-covariance-image-transformers
XCiT-S12|2021|0.851|xcit-cross-covariance-image-transformers
TokenLearner L/8 |2021|0.8887|tokenlearner-what-can-8-learned-tokens-do-for
16-TokenLearner B/16 |2021|0.8706999999999999|tokenlearner-what-can-8-learned-tokens-do-for
VOLO-D5|2021|0.871|volo-vision-outlooker-for-visual-recognition
VOLO-D4|2021|0.868|volo-vision-outlooker-for-visual-recognition
VOLO-D3|2021|0.863|volo-vision-outlooker-for-visual-recognition
VOLO-D2|2021|0.86|volo-vision-outlooker-for-visual-recognition
VOLO-D1|2021|0.852|volo-vision-outlooker-for-visual-recognition
PVTv2-B4|2021|0.838|pvtv2-improved-baselines-with-pyramid-vision
PVTv2-B3|2021|0.8320000000000001|pvtv2-improved-baselines-with-pyramid-vision
PVTv2-B2|2021|0.82|pvtv2-improved-baselines-with-pyramid-vision
PVTv2-B1|2021|0.787|pvtv2-improved-baselines-with-pyramid-vision
PVTv2-B0|2021|0.705|pvtv2-improved-baselines-with-pyramid-vision
GFNet-H-B|2021|0.8290000000000001|global-filter-networks-for-image
AutoFormer-base|2021|0.8240000000000001|autoformer-searching-transformers-for-visual
AutoFormer-small|2021|0.8170000000000001|autoformer-searching-transformers-for-visual
AutoFormer-tiny|2021|0.747|autoformer-searching-transformers-for-visual
GLiT-Bases|2021|0.823|glit-neural-architecture-search-for-global
GLiT-Smalls|2021|0.805|glit-neural-architecture-search-for-global
GLiT-Tinys|2021|0.763|glit-neural-architecture-search-for-global
CoE-Large + CondConv|2021|0.815|collaboration-of-experts-achieving-80-top-1
CoE-Large|2021|0.807|collaboration-of-experts-achieving-80-top-1
CoE-Small + CondConv + PWLU|2021|0.8|collaboration-of-experts-achieving-80-top-1
"ViP-B|384"|2021|0.8420000000000001|visual-parser-representing-part-whole
CycleMLP-B5|2021|0.8320000000000001|cyclemlp-a-mlp-like-architecture-for-dense
SkipblockNet-L|2021|0.7709999999999999|bias-loss-for-mobile-neural-networks
SkipblockNet-M|2021|0.762|bias-loss-for-mobile-neural-networks
WideNet-H|2021|0.8009000000000001|go-wider-instead-of-deeper
WideNet-L|2021|0.7948999999999999|go-wider-instead-of-deeper
WideNet-B|2021|0.7754000000000001|go-wider-instead-of-deeper
SE-CoTNetD-152|2021|0.846|contextual-transformer-networks-for-visual
SE-CoTNetD-101|2021|0.8320000000000001|contextual-transformer-networks-for-visual
ResNet-200|2021|0.818|parametric-contrastive-learning
SE-CoTNetD-50|2021|0.816|contextual-transformer-networks-for-visual
ResNet-101|2021|0.809|parametric-contrastive-learning
DeiT-S with iRPE-QKV|2021|0.8140000000000001|rethinking-and-improving-relative-position
DeiT-S with iRPE-QK|2021|0.8109999999999999|rethinking-and-improving-relative-position
DeiT-S with iRPE-K|2021|0.809|rethinking-and-improving-relative-position
DeiT-Ti with iRPE-K|2021|0.737|rethinking-and-improving-relative-position
Evo-LeViT-384*|2021|0.8220000000000001|evo-vit-slow-fast-token-evolution-for-dynamic
Co-ResNet-152|2021|0.7903|contextual-convolutional-neural-networks
ConvMLP-L|2021|0.802|convmlp-hierarchical-convolutional-mlps-for
ConvMLP-M|2021|0.79|convmlp-hierarchical-convolutional-mlps-for
ConvMLP-S|2021|0.768|convmlp-hierarchical-convolutional-mlps-for
sMLPNet-B |2021|0.8340000000000001|sparse-mlp-for-image-recognition-is-self
sMLPNet-S |2021|0.831|sparse-mlp-for-image-recognition-is-self
sMLPNet-T |2021|0.8190000000000001|sparse-mlp-for-image-recognition-is-self
NASViT |2021|0.8290000000000001|nasvit-neural-architecture-search-for
ConvMixer-1536/20|2021|0.8220000000000001|patches-are-all-you-need-1
NASViT-A5|2021|0.818|nasvit-neural-architecture-search-for
NASViT-A4|2021|0.8140000000000001|nasvit-neural-architecture-search-for
NASViT-A3|2021|0.81|nasvit-neural-architecture-search-for
NASViT-A2|2021|0.805|nasvit-neural-architecture-search-for
DAFT-conv |2021|0.802|a-dot-product-attention-free-transformer
DAFT-full|2021|0.7979999999999999|a-dot-product-attention-free-transformer
NASViT-A1|2021|0.797|nasvit-neural-architecture-search-for
NASViT-A0|2021|0.782|nasvit-neural-architecture-search-for
DeiT-S |2021|0.804|resnet-strikes-back-an-improved-training
ResNet50 |2021|0.804|resnet-strikes-back-an-improved-training
ResNet50 |2021|0.7809999999999999|resnet-strikes-back-an-improved-training
MobileViT-S|2021|0.784|mobilevit-light-weight-general-purpose-and
MobileViT-XS|2021|0.748|mobilevit-light-weight-general-purpose-and
UniNet-B5|2021|0.852|uninet-unified-architecture-search-with
UniNet-B4|2021|0.8420000000000001|uninet-unified-architecture-search-with
UniNet-B2|2021|0.8270000000000001|uninet-unified-architecture-search-with
UniNet-B1|2021|0.804|uninet-unified-architecture-search-with
UniNet-B0|2021|0.7909999999999999|uninet-unified-architecture-search-with
HRFormer-B|2021|0.828|hrformer-high-resolution-transformer-for
HRFormer-T|2021|0.785|hrformer-high-resolution-transformer-for
SReT-T|2021|0.7759999999999999|sliced-recursive-transformer-1
SReT-ExT|2021|0.74|sliced-recursive-transformer-1
MAE |2021|0.8690000000000001|masked-autoencoders-are-scalable-vision
MAE |2021|0.8590000000000001|masked-autoencoders-are-scalable-vision
MAE |2021|0.836|masked-autoencoders-are-scalable-vision
SwinV2-G|2021|0.9017000000000001|swin-transformer-v2-scaling-up-capacity-and
SwinV2-B|2021|0.871|swin-transformer-v2-scaling-up-capacity-and
FBNetV5-F-CLS|2021|0.841|fbnetv5-neural-architecture-search-for
FBNetV5-C-CLS|2021|0.826|fbnetv5-neural-architecture-search-for
FBNetV5|2021|0.818|fbnetv5-neural-architecture-search-for
FBNetV5-A-CLS|2021|0.8170000000000001|fbnetv5-neural-architecture-search-for
FBNetV5-AC-CLS|2021|0.784|fbnetv5-neural-architecture-search-for
FBNetV5-AR-CLS|2021|0.772|fbnetv5-neural-architecture-search-for
DiscreteViT|2021|0.8506999999999999|discrete-representations-strengthen-vision-1
Florence-CoSwin-H|2021|0.9005|florence-a-new-foundation-model-for-computer
MetaFormer PoolFormer-M48|2021|0.825|metaformer-is-actually-what-you-need-for
ResNet-50 |2021|0.7940999999999999|boosting-discriminative-visual-representation
ResNet-18 |2021|0.7232999999999999|boosting-discriminative-visual-representation
MViTv2-H |2021|0.88|improved-multiscale-vision-transformers-for
MViTv2-T|2021|0.823|improved-multiscale-vision-transformers-for
SReT-LT |2021|0.787|a-fast-knowledge-distillation-framework-for
QnA-ViT-Base|2021|0.8370000000000001|learned-queries-for-efficient-local-attention
QnA-ViT-Small|2021|0.8320000000000001|learned-queries-for-efficient-local-attention
RepMLPNet-L256|2021|0.818|repmlpnet-hierarchical-vision-mlp-with-re
QnA-ViT-Tiny|2021|0.8170000000000001|learned-queries-for-efficient-local-attention
ELSA-VOLO-D1|2021|0.847|elsa-enhanced-local-self-attention-for-vision
ELSA-Swin-T|2021|0.8270000000000001|elsa-enhanced-local-self-attention-for-vision
PatchConvNet-L120-21k-384|2021|0.871|augmenting-convolutional-networks-with
PatchConvNet-S60-21k-512|2021|0.8540000000000001|augmenting-convolutional-networks-with
PatchConvNet-B120|2021|0.841|augmenting-convolutional-networks-with
PatchConvNet-B60|2021|0.835|augmenting-convolutional-networks-with
PatchConvNet-S120|2021|0.8320000000000001|augmenting-convolutional-networks-with
PatchConvNet-S60|2021|0.821|augmenting-convolutional-networks-with
DAT-S|2022|0.8370000000000001|vision-transformer-with-deformable-attention
DAT-T|2022|0.82|vision-transformer-with-deformable-attention
ConvNeXt-XL |2022|0.878|a-convnet-for-the-2020s
ConvNeXt-L |2022|0.855|a-convnet-for-the-2020s
ConvNeXt-T|2022|0.821|a-convnet-for-the-2020s
SWAG |2022|0.8859999999999999|revisiting-weakly-supervised-pre-training-of
Omnivore |2022|0.86|omnivore-a-single-model-for-many-visual
Omnivore |2022|0.853|omnivore-a-single-model-for-many-visual
UniFormer-L |2022|0.863|uniformer-unifying-convolution-and-self
UniFormer-L|2022|0.856|uniformer-unifying-convolution-and-self
UniFormer-S|2022|0.8340000000000001|uniformer-unifying-convolution-and-self
Shift-B|2022|0.833|when-shift-operation-meets-vision-transformer
Shift-S|2022|0.828|when-shift-operation-meets-vision-transformer
Shift-T|2022|0.8170000000000001|when-shift-operation-meets-vision-transformer
data2vec |2022|0.866|data2vec-a-general-framework-for-self-1
MKD ViT-L|2022|0.865|meta-knowledge-distillation
SEER |2022|0.858|vision-models-are-more-robust-and-fair-when
MKD ViT-B|2022|0.851|meta-knowledge-distillation
MKD ViT-S|2022|0.831|meta-knowledge-distillation
MKD ViT-T|2022|0.7709999999999999|meta-knowledge-distillation
VAN-B5 |2022|0.863|visual-attention-network
VAN-B4 |2022|0.857|visual-attention-network
VAN-B2|2022|0.828|visual-attention-network
VAN-B1|2022|0.8109999999999999|visual-attention-network
VAN-B0|2022|0.754|visual-attention-network
ViTAE-H + MAE |2022|0.885|vitaev2-vision-transformer-advanced-by
EdgeFormer-S|2022|0.7863|edgeformer-improving-light-weight-convnets-by
Model soups |2022|0.9094|model-soups-averaging-weights-of-multiple
ActiveMLP-L|2022|0.848|activemlp-an-mlp-like-architecture-with
ActiveMLP-T|2022|0.82|activemlp-an-mlp-like-architecture-with
RepLKNet-XL|2022|0.878|scaling-up-your-kernels-to-31x31-revisiting
ViT-L@384 |2022|0.855|three-things-everyone-should-know-about
ViT-B@384 |2022|0.843|three-things-everyone-should-know-about
ViT-B-36x1|2022|0.841|three-things-everyone-should-know-about
ViT-B-18x2|2022|0.841|three-things-everyone-should-know-about
ViT-B |2022|0.8340000000000001|three-things-everyone-should-know-about
ViT-S-24x2|2022|0.826|three-things-everyone-should-know-about
ViT-S-48x1|2022|0.823|three-things-everyone-should-know-about
ELP |2022|0.7613|a-simple-episodic-linear-probe-improves
kNN-CLIP|2022|0.7979999999999999|revisiting-a-knn-based-image-classification
MaxViT-L |2022|0.867|maxvit-multi-axis-vision-transformer
MaxViT-L |2022|0.8640000000000001|maxvit-multi-axis-vision-transformer
MaxViT-B |2022|0.8634000000000001|maxvit-multi-axis-vision-transformer
MaxViT-S |2022|0.8619|maxvit-multi-axis-vision-transformer
MaxViT-T|2022|0.8572|maxvit-multi-axis-vision-transformer
MaxViT-T |2022|0.8523999999999999|maxvit-multi-axis-vision-transformer
MaxViT-B |2022|0.8495|maxvit-multi-axis-vision-transformer
MaxViT-S |2022|0.8445|maxvit-multi-axis-vision-transformer
MaxViT-T |2022|0.8362|maxvit-multi-axis-vision-transformer
DaViT-G|2022|0.904|davit-dual-attention-vision-transformers
DaViT-H|2022|0.902|davit-dual-attention-vision-transformers
DaViT-L |2022|0.875|davit-dual-attention-vision-transformers
DaViT-B |2022|0.8690000000000001|davit-dual-attention-vision-transformers
DaViT-B|2022|0.846|davit-dual-attention-vision-transformers
DaViT-T|2022|0.828|davit-dual-attention-vision-transformers
ViT-L @384 |2022|0.858|deit-iii-revenge-of-the-vit
ViT-H @224 |2022|0.852|deit-iii-revenge-of-the-vit
ViT-B @384 |2022|0.85|deit-iii-revenge-of-the-vit
ViT-L @224 |2022|0.8490000000000001|deit-iii-revenge-of-the-vit
NAT-Base|2022|0.843|neighborhood-attention-transformer
ViT-B @224 |2022|0.838|deit-iii-revenge-of-the-vit
NAT-Small|2022|0.8370000000000001|neighborhood-attention-transformer
ViT-S @384 |2022|0.8340000000000001|deit-iii-revenge-of-the-vit
NAT-Tiny|2022|0.8320000000000001|neighborhood-attention-transformer
NAT-Mini|2022|0.818|neighborhood-attention-transformer
ViT-S @224 |2022|0.8140000000000001|deit-iii-revenge-of-the-vit
FAN-L-Hybrid++|2022|0.871|understanding-the-robustness-in-vision
ASF-former-B|2022|0.8390000000000001|adaptive-split-fusion-transformer
ASF-former-S|2022|0.8270000000000001|adaptive-split-fusion-transformer
CoCa |2022|0.9059999999999999|coca-contrastive-captioners-are-image-text
Sequencer2D-L\u2191392|2022|0.846|sequencer-deep-lstm-for-image-classification
Sequencer2D-L|2022|0.8340000000000001|sequencer-deep-lstm-for-image-classification
Sequencer2D-M|2022|0.828|sequencer-deep-lstm-for-image-classification
Sequencer2D-S|2022|0.823|sequencer-deep-lstm-for-image-classification
CLCNet |2022|0.8661|clcnet-rethinking-of-ensemble-modeling-with
CLCNet |2022|0.8645999999999999|clcnet-rethinking-of-ensemble-modeling-with
CLCNet |2022|0.8642|clcnet-rethinking-of-ensemble-modeling-with
CLCNet |2022|0.8528|clcnet-rethinking-of-ensemble-modeling-with
CLCNet |2022|0.8388|clcnet-rethinking-of-ensemble-modeling-with
Bamboo |2022|0.871|deeper-vs-wider-a-revisit-of-transformer
Bamboo |2022|0.863|deeper-vs-wider-a-revisit-of-transformer
Bamboo |2022|0.8420000000000001|deeper-vs-wider-a-revisit-of-transformer
\u00b52Net |2022|0.8674|an-evolutionary-approach-to-dynamic
MixMIM-B|2022|0.851|mixmim-mixed-and-masked-image-modeling-for
"LITv2-B|384"|2022|0.847|fast-vision-transformers-with-hilo-attention
TransBoost-ViT-S|2022|0.8367|transboost-improving-the-best-imagenet
LITv2-B|2022|0.836|fast-vision-transformers-with-hilo-attention
LITv2-M|2022|0.833|fast-vision-transformers-with-hilo-attention
TransBoost-ConvNext-T|2022|0.8245999999999999|transboost-improving-the-best-imagenet
TransBoost-Swin-T|2022|0.8216|transboost-improving-the-best-imagenet
LITv2-S|2022|0.82|fast-vision-transformers-with-hilo-attention
TransBoost-ResNet50-StrikesBack|2022|0.8115000000000001|transboost-improving-the-best-imagenet
TransBoost-ResNet152|2022|0.8064|transboost-improving-the-best-imagenet
TransBoost-ResNet101|2022|0.7986|transboost-improving-the-best-imagenet
TransBoost-ResNet50|2022|0.7903|transboost-improving-the-best-imagenet
TransBoost-EfficientNetB0|2022|0.7859999999999999|transboost-improving-the-best-imagenet
TransBoost-ResNet34|2022|0.767|transboost-improving-the-best-imagenet
TransBoost-ResNet18|2022|0.7336|transboost-improving-the-best-imagenet
FD |2022|0.89|contrastive-learning-rivals-masked-image
WaveMix-192/16 |2022|0.7493000000000001|wavemix-lite-a-resource-efficient-neural
EfficientViT-B3 |2022|0.8420000000000001|efficientvit-enhanced-linear-attention-for
Pyramid ViG-B|2022|0.8370000000000001|vision-gnn-an-image-is-worth-graph-of-nodes
Pyramid ViG-M|2022|0.831|vision-gnn-an-image-is-worth-graph-of-nodes
Pyramid ViG-S|2022|0.821|vision-gnn-an-image-is-worth-graph-of-nodes
Pyramid ViG-Ti|2022|0.782|vision-gnn-an-image-is-worth-graph-of-nodes
MobileOne-S4 |2022|0.8140000000000001|an-improved-one-millisecond-mobile-backbone
MobileOne-S3 |2022|0.8|an-improved-one-millisecond-mobile-backbone
MobileOne-S4|2022|0.794|an-improved-one-millisecond-mobile-backbone
MobileOne-S2 |2022|0.7909999999999999|an-improved-one-millisecond-mobile-backbone
MobileOne-S3|2022|0.7809999999999999|an-improved-one-millisecond-mobile-backbone
MobileOne-S2|2022|0.774|an-improved-one-millisecond-mobile-backbone
MobileOne-S1 |2022|0.774|an-improved-one-millisecond-mobile-backbone
MobileOne-S1|2022|0.759|an-improved-one-millisecond-mobile-backbone
MobileOne-S0 |2022|0.725|an-improved-one-millisecond-mobile-backbone
MobileOne-S0|2022|0.7140000000000001|an-improved-one-millisecond-mobile-backbone
Top-k DiffSortNets |2022|0.8837|differentiable-top-k-classification-learning-1
"Our SP-ViT-L|384"|2022|0.863|sp-vit-learning-2d-spatial-priors-for-vision
"Our SP-ViT-M|384"|2022|0.86|sp-vit-learning-2d-spatial-priors-for-vision
Our SP-ViT-L|2022|0.855|sp-vit-learning-2d-spatial-priors-for-vision
"SP-ViT-S|384"|2022|0.851|sp-vit-learning-2d-spatial-priors-for-vision
Our SP-ViT-M|2022|0.8490000000000001|sp-vit-learning-2d-spatial-priors-for-vision
Our SP-ViT-S|2022|0.8390000000000001|sp-vit-learning-2d-spatial-priors-for-vision
GC ViT-B|2022|0.845|global-context-vision-transformers
GC ViT-S|2022|0.84|global-context-vision-transformers
GC ViT-T|2022|0.8340000000000001|global-context-vision-transformers
GC ViT-XT|2022|0.82|global-context-vision-transformers
GC ViT-XXT|2022|0.7979999999999999|global-context-vision-transformers
EdgeNeXt-S|2022|0.794|edgenext-efficiently-amalgamated-cnn
EdgeNeXt-XXS|2022|0.7120000000000001|edgenext-efficiently-amalgamated-cnn
RevBiFPN-S5|2022|0.8370000000000001|revbifpn-the-fully-reversible-bidirectional
RevBiFPN-S4|2022|0.83|revbifpn-the-fully-reversible-bidirectional
RevBiFPN-S3|2022|0.8109999999999999|revbifpn-the-fully-reversible-bidirectional
RevBiFPN-S2|2022|0.79|revbifpn-the-fully-reversible-bidirectional
RevBiFPN-S1|2022|0.759|revbifpn-the-fully-reversible-bidirectional
RevBiFPN-S0|2022|0.728|revbifpn-the-fully-reversible-bidirectional
Wave-ViT-L|2022|0.855|wave-vit-unifying-wavelet-and-transformers
Wave-ViT-B|2022|0.848|wave-vit-unifying-wavelet-and-transformers
Wave-ViT-S|2022|0.8390000000000001|wave-vit-unifying-wavelet-and-transformers
UniNet-B6|2022|0.8740000000000001|uninet-unified-architecture-search-with-1
UniNet-B5|2022|0.87|uninet-unified-architecture-search-with-1
Next-ViT-L @384|2022|0.847|next-vit-next-generation-vision-transformer
UniNet-B0|2022|0.8079999999999999|uninet-unified-architecture-search-with-1
TinyViT-21M-distill |2022|0.848|tinyvit-fast-pretraining-distillation-for
TinyViT-11M-distill |2022|0.8320000000000001|tinyvit-fast-pretraining-distillation-for
TinyViT-21M|2022|0.831|tinyvit-fast-pretraining-distillation-for
TinyViT-11M|2022|0.815|tinyvit-fast-pretraining-distillation-for
TinyViT-5M-distill |2022|0.807|tinyvit-fast-pretraining-distillation-for
TinyViT-5M|2022|0.7909999999999999|tinyvit-fast-pretraining-distillation-for
ResMLP-B24 + STD|2022|0.8240000000000001|spatial-channel-token-distillation-for-vision
CycleMLP-B2 + STD|2022|0.821|spatial-channel-token-distillation-for-vision
Mixer-S16 + STD|2022|0.757|spatial-channel-token-distillation-for-vision
HorNet-L |2022|0.877|hornet-efficient-high-order-spatial
gSwin-S|2022|0.8301000000000001|gswin-gated-mlp-vision-model-with
gSwin-T|2022|0.8170999999999999|gswin-gated-mlp-vision-model-with
gSwin-VT|2022|0.8031999999999999|gswin-gated-mlp-vision-model-with
dBOT ViT-H |2022|0.882|exploring-target-representations-for-masked
dBOT ViT-L |2022|0.878|exploring-target-representations-for-masked
dBOT ViT-B |2022|0.857|exploring-target-representations-for-masked
ViT-e|2022|0.909|pali-a-jointly-scaled-multilingual-language
MAE+DAT |2022|0.8702|enhance-the-visual-representation-via
Mega|2022|0.8240000000000001|mega-moving-average-equipped-gated-attention
GPaCo |2022|0.8601000000000001|generalized-parametric-contrastive-learning
GPaCo |2022|0.84|generalized-parametric-contrastive-learning
GPaCo |2022|0.797|generalized-parametric-contrastive-learning
DiNAT-Large |2022|0.875|dilated-neighborhood-attention-transformer
DiNAT-Large |2022|0.8740000000000001|dilated-neighborhood-attention-transformer
DiNAT_s-Large |2022|0.865|dilated-neighborhood-attention-transformer
DiNAT-Base|2022|0.8440000000000001|dilated-neighborhood-attention-transformer
DiNAT-Small|2022|0.838|dilated-neighborhood-attention-transformer
DiNAT-Tiny|2022|0.8270000000000001|dilated-neighborhood-attention-transformer
DiNAT-Mini|2022|0.818|dilated-neighborhood-attention-transformer
MobileViTv3-S|2022|0.7929999999999999|mobilevitv3-mobile-friendly-vision
MobileViTv3-1.0|2022|0.7864|mobilevitv3-mobile-friendly-vision
MobileViTv3-XS|2022|0.767|mobilevitv3-mobile-friendly-vision
MobileViTv3-0.75|2022|0.7655|mobilevitv3-mobile-friendly-vision
MobileViTv3-0.5|2022|0.7232999999999999|mobilevitv3-mobile-friendly-vision
MobileViTv3-XXS|2022|0.7098|mobilevitv3-mobile-friendly-vision
MOAT-4 22K+1K|2022|0.8909999999999999|moat-alternating-mobile-convolution-and
MOAT-3 1K only|2022|0.867|moat-alternating-mobile-convolution-and
MOAT-0 1K only|2022|0.833|moat-alternating-mobile-convolution-and
OTTT|2022|0.6515000000000001|online-training-through-time-for-spiking
WaveMixLite-256/24|2022|0.677|wavemix-lite-a-resource-efficient-neural-1
ViT-B/16-224+HTM|2022|0.8237000000000001|tokenmixup-efficient-attention-guided-token
CAFormer-B36 |2022|0.8640000000000001|metaformer-baselines-for-vision
CAFormer-M36 |2022|0.862|metaformer-baselines-for-vision
CAFormer-S36 |2022|0.857|metaformer-baselines-for-vision
ConvFormer-B36 |2022|0.857|metaformer-baselines-for-vision
ConvFormer-M36 |2022|0.856|metaformer-baselines-for-vision
CAFormer-B36 |2022|0.855|metaformer-baselines-for-vision
ConvFormer-S36 |2022|0.8540000000000001|metaformer-baselines-for-vision
CAFormer-M36 |2022|0.852|metaformer-baselines-for-vision
CAFormer-S18 |2022|0.85|metaformer-baselines-for-vision
ConvFormer-B36 |2022|0.848|metaformer-baselines-for-vision
CAFormer-S36 |2022|0.845|metaformer-baselines-for-vision
ConvFormer-M36 |2022|0.845|metaformer-baselines-for-vision
ConvFormer-S18 |2022|0.8440000000000001|metaformer-baselines-for-vision
ConvFormer-S36 |2022|0.841|metaformer-baselines-for-vision
CAFormer-S18 |2022|0.836|metaformer-baselines-for-vision
ConvFormer-S18 |2022|0.83|metaformer-baselines-for-vision
MogaNet-XL |2022|0.878|efficient-multi-order-gated-aggregation
MogaNet-L|2022|0.847|efficient-multi-order-gated-aggregation
MogaNet-B|2022|0.843|efficient-multi-order-gated-aggregation
MogaNet-S|2022|0.8340000000000001|efficient-multi-order-gated-aggregation
MogaNet-T |2022|0.8|efficient-multi-order-gated-aggregation
MogaNet-XT |2022|0.772|efficient-multi-order-gated-aggregation
InternImage-DCNv3-G |2022|0.9009999999999999|internimage-exploring-large-scale-vision
InternImage-H|2022|0.8959999999999999|internimage-exploring-large-scale-vision
InternImage-XL|2022|0.88|internimage-exploring-large-scale-vision
InternImage-L|2022|0.877|internimage-exploring-large-scale-vision
InternImage-B|2022|0.8490000000000001|internimage-exploring-large-scale-vision
InternImage-S|2022|0.8420000000000001|internimage-exploring-large-scale-vision
InternImage-T|2022|0.835|internimage-exploring-large-scale-vision
EVA|2022|0.897|eva-exploring-the-limits-of-masked-visual
M3I Pre-training |2022|0.8959999999999999|towards-all-in-one-pre-training-via
Heinsen Routing + BEiT-large 16 224|2022|0.867|an-algorithm-for-routing-vectors-in-sequences
Last Layer Tuning with Newton Step |2022|0.889|differentially-private-image-classification
SALG-ST|2022|0.759|semantic-aware-local-global-vision
PAT-B|2022|0.836|pattern-attention-transformer-with-doughnut
PAT-S|2022|0.831|pattern-attention-transformer-with-doughnut
IPT-B|2022|0.836|incepformer-efficient-inception-transformer
IPT-S|2022|0.8290000000000001|incepformer-efficient-inception-transformer
IPT-T|2022|0.805|incepformer-efficient-inception-transformer
ViT-H@224 |2022|0.88|co-training-2-l-submodels-for-visual
ViT-L@224 |2022|0.875|co-training-2-l-submodels-for-visual
Swin-L@224 |2022|0.871|co-training-2-l-submodels-for-visual
ViT-B@224 |2022|0.863|co-training-2-l-submodels-for-visual
Swin-B@224 |2022|0.862|co-training-2-l-submodels-for-visual
ConvNeXt-B@224 |2022|0.858|co-training-2-l-submodels-for-visual
PiT-B@224 |2022|0.858|co-training-2-l-submodels-for-visual
ViT-M@224 |2022|0.85|co-training-2-l-submodels-for-visual
RegnetY16GF@224 |2022|0.8420000000000001|co-training-2-l-submodels-for-visual
ViT-S@224 |2022|0.831|co-training-2-l-submodels-for-visual
R-Mix |2022|0.7739|expeditious-saliency-guided-mix-up-through
OpenCLIP ViT-H/14|2022|0.885|reproducible-scaling-laws-for-contrastive
data2vec 2.0|2022|0.8740000000000001|efficient-self-supervised-learning-with
NEXcepTion-S|2022|0.82|from-xception-to-nexception-new-design
NEXcepTion-TP|2022|0.818|from-xception-to-nexception-new-design
NEXcepTion-T|2022|0.815|from-xception-to-nexception-new-design
Adlik-ViT-SG+Swin_large+Convnext_xlarge|2022|0.8835999999999999|a-convnet-for-the-2020s
RevCol-H|2022|0.9|reversible-column-networks
mPLUG-2|2023|0.885|mplug-2-a-modularized-multi-modal-foundation
DeepMAD-89M|2023|0.84|deepmad-mathematical-architecture-design-for
BiFormer-B* |2023|0.8540000000000001|biformer-vision-transformer-with-bi-level
BiFormer-S* |2023|0.843|biformer-vision-transformer-with-bi-level
BiFormer-T |2023|0.8140000000000001|biformer-vision-transformer-with-bi-level
ViC-MAE |2023|0.85|visual-representation-learning-from-unlabeled
MAWS |2023|0.9009999999999999|the-effectiveness-of-mae-pre-pretraining-for
MAWS |2023|0.898|the-effectiveness-of-mae-pre-pretraining-for
MAWS |2023|0.895|the-effectiveness-of-mae-pre-pretraining-for
MAWS |2023|0.888|the-effectiveness-of-mae-pre-pretraining-for
MAWS |2023|0.868|the-effectiveness-of-mae-pre-pretraining-for
Diffusion Classifier|2023|0.7909999999999999|your-diffusion-model-is-secretly-a-zero-shot
CloFormer-S|2023|0.816|rethinking-local-perception-in-lightweight
CloFormer-XS|2023|0.7979999999999999|rethinking-local-perception-in-lightweight
CloFormer-XXS|2023|0.77|rethinking-local-perception-in-lightweight
Unicom |2023|0.883|unicom-universal-and-compact-representation
XCiT-M |2023|0.841|mixpro-data-augmentation-with-maskmix-and
CA-Swin-S |2023|0.8370000000000001|mixpro-data-augmentation-with-maskmix-and
DeiT-B |2023|0.8290000000000001|mixpro-data-augmentation-with-maskmix-and
CA-Swin-T |2023|0.828|mixpro-data-augmentation-with-maskmix-and
PVT-M |2023|0.8270000000000001|mixpro-data-augmentation-with-maskmix-and
PVT-S |2023|0.812|mixpro-data-augmentation-with-maskmix-and
CaiT-XXS |2023|0.8059999999999999|mixpro-data-augmentation-with-maskmix-and
PVT-T |2023|0.767|mixpro-data-augmentation-with-maskmix-and
DeiT-T |2023|0.738|mixpro-data-augmentation-with-maskmix-and
ONE-PEACE|2023|0.898|one-peace-exploring-one-general
Hiera-H|2023|0.8690000000000001|hiera-a-hierarchical-vision-transformer
Swin-T+SSA|2023|0.8189|the-information-pathways-hypothesis
ViT-H @224 |2023|0.857|augmenting-sub-model-to-improve-main-model
ViT-L @224 |2023|0.853|augmenting-sub-model-to-improve-main-model
ViT-B @224 |2023|0.8420000000000001|augmenting-sub-model-to-improve-main-model
CaiT-S24|2023|0.8491|which-transformer-to-favor-a-comparative
XCiT-S|2023|0.8365|which-transformer-to-favor-a-comparative
Wave-ViT-S|2023|0.8361|which-transformer-to-favor-a-comparative
SwinV2-Ti|2023|0.8309000000000001|which-transformer-to-favor-a-comparative
ViT-S|2023|0.8254|which-transformer-to-favor-a-comparative
EViT |2023|0.8229000000000001|which-transformer-to-favor-a-comparative
STViT-Swin-Ti|2023|0.8222|which-transformer-to-favor-a-comparative
ToMe-ViT-S|2023|0.8210999999999999|which-transformer-to-favor-a-comparative
EViT |2023|0.8195999999999999|which-transformer-to-favor-a-comparative
GFNet-S|2023|0.8133|which-transformer-to-favor-a-comparative
DynamicViT-S|2023|0.8109000000000001|which-transformer-to-favor-a-comparative
TokenLearner-ViT-8|2023|0.8066|which-transformer-to-favor-a-comparative
CoaT-Ti|2023|0.7842|which-transformer-to-favor-a-comparative
Poly-SA-ViT-S|2023|0.7834|which-transformer-to-favor-a-comparative
EfficientFormer-V2-S0|2023|0.7153|which-transformer-to-favor-a-comparative
DAT-B++ |2023|0.8590000000000001|dat-spatially-dynamic-vision-transformer-with
DAT-B++ |2023|0.8490000000000001|dat-spatially-dynamic-vision-transformer-with
DAT-S++|2023|0.846|dat-spatially-dynamic-vision-transformer-with
DAT-T++|2023|0.8390000000000001|dat-spatially-dynamic-vision-transformer-with
MIRL|2023|0.848|masked-image-residual-learning-for-scaling-1
MIRL |2023|0.862|masked-image-residual-learning-for-scaling-1
GTP-ViT-B-Patch8/P20|2023|0.858|gtp-vit-efficient-vision-transformers-via
GTP-EVA-L/P8|2023|0.8540000000000001|gtp-vit-efficient-vision-transformers-via
GTP-ViT-L/P8|2023|0.8370000000000001|gtp-vit-efficient-vision-transformers-via
GTP-LV-ViT-M/P8|2023|0.828|gtp-vit-efficient-vision-transformers-via
GTP-LV-ViT-S/P8|2023|0.8190000000000001|gtp-vit-efficient-vision-transformers-via
GTP-DeiT-B/P8|2023|0.815|gtp-vit-efficient-vision-transformers-via
GTP-DeiT-S/P8|2023|0.795|gtp-vit-efficient-vision-transformers-via
UniRepLKNet-XL++|2023|0.88|unireplknet-a-universal-perception-large
UniRepLKNet-L++|2023|0.879|unireplknet-a-universal-perception-large
UniRepLKNet-B++|2023|0.8740000000000001|unireplknet-a-universal-perception-large
UniRepLKNet-S++|2023|0.8640000000000001|unireplknet-a-universal-perception-large
UniRepLKNet-S|2023|0.8390000000000001|unireplknet-a-universal-perception-large
UniRepLKNet-T|2023|0.8320000000000001|unireplknet-a-universal-perception-large
UniRepLKNet-N|2023|0.816|unireplknet-a-universal-perception-large
UniRepLKNet-P|2023|0.802|unireplknet-a-universal-perception-large
UniRepLKNet-F|2023|0.7859999999999999|unireplknet-a-universal-perception-large
UniRepLKNet-A|2023|0.77|unireplknet-a-universal-perception-large
